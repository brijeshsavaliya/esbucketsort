[2016-03-30 22:57:52,597][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=78,reason=Function not implemented
[2016-03-30 22:57:52,599][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.
[2016-03-30 22:58:20,119][WARN ][org.elasticsearch.cluster] [node_t0] Failed to execute NodeStatsAction for ClusterInfoUpdateJob
ElasticsearchException[force exception on [cluster:monitor/nodes/stats]]
	at org.elasticsearch.cluster.ClusterInfoServiceIT$BlockingActionFilter.apply(ClusterInfoServiceIT.java:105)
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:63)
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:99)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:77)
	at org.elasticsearch.cluster.InternalClusterInfoService.updateNodeStats(InternalClusterInfoService.java:271)
	at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:307)
	at org.elasticsearch.cluster.ClusterInfoServiceIT.testClusterInfoServiceInformationClearOnError(ClusterInfoServiceIT.java:250)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:58:20,135][WARN ][org.elasticsearch.cluster] [node_t0] Failed to execute IndicesStatsAction for ClusterInfoUpdateJob
ElasticsearchException[force exception on [indices:monitor/stats]]
	at org.elasticsearch.cluster.ClusterInfoServiceIT$BlockingActionFilter.apply(ClusterInfoServiceIT.java:105)
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:63)
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:99)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:77)
	at org.elasticsearch.cluster.InternalClusterInfoService.updateIndicesStats(InternalClusterInfoService.java:285)
	at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:336)
	at org.elasticsearch.cluster.ClusterInfoServiceIT.testClusterInfoServiceInformationClearOnError(ClusterInfoServiceIT.java:250)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:58:40,262][WARN ][org.elasticsearch.discovery.zen] [node_t3] master left (reason = shut_down), current nodes: {{node_t1}{bVQhG_-TQ3uiNi3xuYu6AA}{127.0.0.1}{127.0.0.1:30101}[mode=>network],{node_t2}{-L8glJcGSfeN4pgHkFhScw}{127.0.0.1}{127.0.0.1:30102}[mode=>network],{node_t3}{03ry3JdjS3CKWgZpCa_jgQ}{127.0.0.1}{127.0.0.1:30103}[mode=>network],}
[2016-03-30 22:58:40,264][WARN ][org.elasticsearch.discovery.zen] [node_t2] master left (reason = shut_down), current nodes: {{node_t1}{bVQhG_-TQ3uiNi3xuYu6AA}{127.0.0.1}{127.0.0.1:30101}[mode=>network],{node_t2}{-L8glJcGSfeN4pgHkFhScw}{127.0.0.1}{127.0.0.1:30102}[mode=>network],{node_t3}{03ry3JdjS3CKWgZpCa_jgQ}{127.0.0.1}{127.0.0.1:30103}[mode=>network],}
[2016-03-30 22:58:40,265][WARN ][org.elasticsearch.discovery.zen] [node_t1] master left (reason = shut_down), current nodes: {{node_t1}{bVQhG_-TQ3uiNi3xuYu6AA}{127.0.0.1}{127.0.0.1:30101}[mode=>network],{node_t2}{-L8glJcGSfeN4pgHkFhScw}{127.0.0.1}{127.0.0.1:30102}[mode=>network],{node_t3}{03ry3JdjS3CKWgZpCa_jgQ}{127.0.0.1}{127.0.0.1:30103}[mode=>network],}
[2016-03-30 22:58:43,815][WARN ][org.elasticsearch.discovery.zen] [node_t2] master left (reason = shut_down), current nodes: {{node_t1}{yQobR_kaQIW-L971lOfeFA}{127.0.0.1}{127.0.0.1:30105}[mode=>network],{node_t2}{FH100_E5QEW5HASwgIO-qA}{127.0.0.1}{127.0.0.1:30106}[mode=>network],}
[2016-03-30 22:58:43,815][WARN ][org.elasticsearch.discovery.zen] [node_t1] master left (reason = shut_down), current nodes: {{node_t1}{yQobR_kaQIW-L971lOfeFA}{127.0.0.1}{127.0.0.1:30105}[mode=>network],{node_t2}{FH100_E5QEW5HASwgIO-qA}{127.0.0.1}{127.0.0.1:30106}[mode=>network],}
[2016-03-30 22:58:43,821][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t2] failed to send ping to [{node_t0}{4t4GpQNwT4ynB8nmrn243A}{127.0.0.1}{127.0.0.1:30104}[mode=>network]]
SendRequestTransportException[[node_t0][127.0.0.1:30104][internal:discovery/zen/unicast]]; nested: NodeNotConnectedException[[node_t0][127.0.0.1:30104] Node not connected];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:857)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:368)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: NodeNotConnectedException[[node_t0][127.0.0.1:30104] Node not connected]
	at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:1128)
	at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:815)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 12 more
[2016-03-30 22:58:48,500][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:48,500][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:48,504][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.5.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.5.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.5.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:48,613][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:48,613][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:48,614][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.5.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.5.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.5.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:48,681][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:48,682][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:48,682][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:48,799][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:48,799][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:48,799][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:48,863][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:48,863][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:48,864][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.7.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.7.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.7.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:48,933][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:48,933][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:48,935][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.8] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.8] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.8] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,007][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:49,007][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,007][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.1.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.1.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.1.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,084][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,084][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.0.rc2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.0.rc2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.0.rc2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,165][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:49,165][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,166][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,256][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:49,256][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,257][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.7.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.7.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.7.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,343][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:49,344][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,344][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,418][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:49,418][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,419][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.6.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.6.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.6.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,506][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,506][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,600][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:49,600][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,601][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.9] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.9] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.9] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,702][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-0.20.6-and-1.1.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-0.20.6-and-1.1.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-0.20.6-and-1.1.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,766][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,766][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,852][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,852][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:49,919][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:49,919][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:49,920][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,004][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:50,004][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,004][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.5] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.5] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.5] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,096][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,096][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.0.beta2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.0.beta2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.0.beta2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,164][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:50,164][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,164][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.6] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.6] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.6] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,245][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:50,245][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,246][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.4] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,339][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:50,339][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,339][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.5.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.5.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.5.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,410][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:50,410][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,411][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,496][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:50,496][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,497][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.5] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.5] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.5] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,598][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:50,598][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,599][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.7.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.7.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.7.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,703][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,703][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.0.rc1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.0.rc1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.0.rc1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,787][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:50,787][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,789][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.7.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.7.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.7.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:50,864][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:50,864][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:50,865][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,005][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,006][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,007][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,073][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,073][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,074][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,184][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,184][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,187][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.4] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,268][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,268][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,268][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.0.beta1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.0.beta1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.0.beta1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,339][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,340][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,340][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,411][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,411][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,412][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.1.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.1.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.1.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,509][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,509][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,512][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.7] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.7] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.7] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,594][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,594][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,596][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.6.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.6.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.6.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,710][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,711][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,781][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,781][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,781][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.4] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,872][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,872][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,873][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.6.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.6.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.6.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:51,965][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:51,965][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:51,966][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.1.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.1.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.1.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:52,026][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:52,027][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:52,027][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:58:52,096][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 22:58:52,096][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 22:58:52,097][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 22:59:02,886][WARN ][org.elasticsearch.bwcompat] Old repositories tests contain extra repo: 2.0.0-beta1
[2016-03-30 22:59:03,528][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:03,738][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:03,887][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:03,969][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:04,016][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:04,255][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:04,279][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:04,292][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:04,316][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:04,336][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:04,353][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:04,372][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:04,399][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:05,118][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] wipe translog location - creating new translog
[2016-03-30 22:59:05,915][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,160][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=31921, numOps=414, translogFileGeneration= 1}
[2016-03-30 22:59:06,171][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,182][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,193][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=kEJbWxGvSQi5q72XtZVa1A], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-03-31T05:59:06.070Z]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,238][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 2}
[2016-03-30 22:59:06,239][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 2}
[2016-03-30 22:59:06,247][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,248][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,250][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=dNyuXyhTQTCaNfl2YGh8aQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.194Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,281][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 3}
[2016-03-30 22:59:06,282][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 3}
[2016-03-30 22:59:06,283][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 3}
[2016-03-30 22:59:06,292][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,293][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,295][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=EkL3SUGQTo6l7768yW6y0Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.251Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,323][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 4}
[2016-03-30 22:59:06,324][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 4}
[2016-03-30 22:59:06,325][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 4}
[2016-03-30 22:59:06,325][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 4}
[2016-03-30 22:59:06,335][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,336][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,338][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=9I8WyQhXRdSYf_FMaBJTYg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.296Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,408][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 5}
[2016-03-30 22:59:06,409][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 5}
[2016-03-30 22:59:06,410][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 5}
[2016-03-30 22:59:06,410][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 5}
[2016-03-30 22:59:06,411][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 5}
[2016-03-30 22:59:06,421][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,424][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,425][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=zhFHLse6QRyGU_86q3yTzg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.339Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,458][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 6}
[2016-03-30 22:59:06,459][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 6}
[2016-03-30 22:59:06,459][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 6}
[2016-03-30 22:59:06,460][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 6}
[2016-03-30 22:59:06,461][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 6}
[2016-03-30 22:59:06,461][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 6}
[2016-03-30 22:59:06,474][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,476][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,477][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=BAM86aBtQyCsocbz3O-Kdw], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.426Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,581][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 7}
[2016-03-30 22:59:06,583][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 7}
[2016-03-30 22:59:06,583][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 7}
[2016-03-30 22:59:06,584][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 7}
[2016-03-30 22:59:06,584][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 7}
[2016-03-30 22:59:06,585][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 7}
[2016-03-30 22:59:06,586][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 7}
[2016-03-30 22:59:06,596][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,606][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,607][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=MdCFlzlPRtmqJ5b2rSkiWw], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.478Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,682][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 8}
[2016-03-30 22:59:06,683][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 8}
[2016-03-30 22:59:06,683][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 8}
[2016-03-30 22:59:06,684][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 8}
[2016-03-30 22:59:06,684][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 8}
[2016-03-30 22:59:06,685][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 8}
[2016-03-30 22:59:06,686][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 8}
[2016-03-30 22:59:06,686][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 8}
[2016-03-30 22:59:06,697][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,708][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,710][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=h7yhpVEvToGcORbwT-KmCA], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.608Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,774][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 9}
[2016-03-30 22:59:06,775][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 9}
[2016-03-30 22:59:06,775][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 9}
[2016-03-30 22:59:06,776][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 9}
[2016-03-30 22:59:06,776][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 9}
[2016-03-30 22:59:06,777][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 9}
[2016-03-30 22:59:06,778][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 9}
[2016-03-30 22:59:06,780][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 9}
[2016-03-30 22:59:06,780][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 9}
[2016-03-30 22:59:06,791][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,808][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,809][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=dZdn0d8WRkKRAI6XZ_Grtg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.711Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,884][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,885][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,886][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,886][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,887][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,887][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,888][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,889][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,889][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,890][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 10}
[2016-03-30 22:59:06,902][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:06,918][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:06,921][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=iaACmsugTcGu-H27HdmBiA], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.811Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:07,002][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,003][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,004][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,005][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,006][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,006][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,007][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,007][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,008][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,009][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,010][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 11}
[2016-03-30 22:59:07,024][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:07,034][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:07,036][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=YJjR-fw4QXaHNta_hi0Idg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:06.948Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:07,080][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,081][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,081][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,082][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,083][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,084][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,085][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,085][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,086][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,086][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,087][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,090][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=43, numOps=0, translogFileGeneration= 12}
[2016-03-30 22:59:07,105][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 22:59:07,110][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:07,111][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[2acjjCBITLuazpOgb8cZMw], [P], v[3], s[INITIALIZING], a[id=Xp7wXp90TEqIs1TW24RoQQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-31T05:59:07.037Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]], indexUUID [tY-1lpcwTDu_K1W4BTAKHA], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$480/679343206.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$479/248889946.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xad21bf6e, got: 0xc8468428]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 22:59:10,582][WARN ][org.elasticsearch.action.index] [node_s0] unexpected error during the primary phase for action [indices:data/write/index], request [index {[test][type1][1], source[{"field":"value1"}]}]
[test] RoutingMissingException[routing is required for [test]/[type1]/[1]]
	at org.elasticsearch.action.index.IndexRequest.process(IndexRequest.java:641)
	at org.elasticsearch.action.index.TransportIndexAction.resolveRequest(TransportIndexAction.java:127)
	at org.elasticsearch.action.index.TransportIndexAction.resolveRequest(TransportIndexAction.java:64)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:421)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:130)
	at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:133)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:117)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:64)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:240)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:237)
	at org.elasticsearch.transport.local.LocalTransport.handleRequest(LocalTransport.java:290)
	at org.elasticsearch.transport.local.LocalTransport.messageReceived(LocalTransport.java:251)
	at org.elasticsearch.transport.local.LocalTransport$1.run(LocalTransport.java:229)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:10,598][WARN ][org.elasticsearch.action.delete] [node_s0] unexpected error during the primary phase for action [indices:data/write/delete], request [delete {[test][type1][1]}]
[test] RoutingMissingException[routing is required for [test]/[type1]/[1]]
	at org.elasticsearch.action.delete.TransportDeleteAction.resolveRequest(TransportDeleteAction.java:109)
	at org.elasticsearch.action.delete.TransportDeleteAction.resolveRequest(TransportDeleteAction.java:53)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:421)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:130)
	at org.elasticsearch.action.delete.TransportDeleteAction.innerExecute(TransportDeleteAction.java:118)
	at org.elasticsearch.action.delete.TransportDeleteAction.doExecute(TransportDeleteAction.java:92)
	at org.elasticsearch.action.delete.TransportDeleteAction.doExecute(TransportDeleteAction.java:53)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:59)
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:337)
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:337)
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:86)
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:60)
	at org.elasticsearch.routing.SimpleRoutingIT.testRequiredRoutingMapping(SimpleRoutingIT.java:182)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:15,693][WARN ][org.elasticsearch.action.index] [node_s1] unexpected error during the primary phase for action [indices:data/write/index], request [index {[alias][type1][1], source[{"field":"value1","routing_field":"0"}]}]
MapperParsingException[The provided routing value [1] doesn't match the routing key stored in the document: [0]]
	at org.elasticsearch.action.index.IndexRequest.process(IndexRequest.java:618)
	at org.elasticsearch.action.index.TransportIndexAction.resolveRequest(TransportIndexAction.java:127)
	at org.elasticsearch.action.index.TransportIndexAction.resolveRequest(TransportIndexAction.java:64)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:421)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:130)
	at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:133)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:117)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:64)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:240)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:237)
	at org.elasticsearch.transport.local.LocalTransport.handleRequest(LocalTransport.java:290)
	at org.elasticsearch.transport.local.LocalTransport.messageReceived(LocalTransport.java:251)
	at org.elasticsearch.transport.local.LocalTransport$1.run(LocalTransport.java:229)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 22:59:51,077][WARN ][org.elasticsearch.index.shard] [node_s2] [test][2] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 22:59:51,078][WARN ][org.elasticsearch.index.shard] [node_s1] [test][3] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 22:59:51,100][WARN ][org.elasticsearch.index.shard] [node_s1] [test][0] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 22:59:51,130][WARN ][org.elasticsearch.index.shard] [node_s0] [test][1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 22:59:51,873][ERROR][org.elasticsearch.update ] Actual version [113] Expected version [113] Total failures [468]
[2016-03-30 22:59:51,875][ERROR][org.elasticsearch.update ] Actual version [1] Expected version [1] Total failures [580]
[2016-03-30 22:59:51,876][ERROR][org.elasticsearch.update ] Actual version [1] Expected version [1] Total failures [580]
[2016-03-30 23:00:08,048][WARN ][org.elasticsearch.discovery.zen] [node_t1] master left (reason = shut_down), current nodes: {{node_t1}{uIOckHIORRyya8hI65JsQQ}{127.0.0.1}{127.0.0.1:9441}[mode=>network],}
[2016-03-30 23:00:36,605][WARN ][org.elasticsearch.test.transport] [node_t1] Transport response handler not found of id [6]
[2016-03-30 23:00:36,616][WARN ][org.elasticsearch.test.transport] [node_t1] Transport response handler not found of id [7]
[2016-03-30 23:00:39,217][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 23:00:44,339][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 23:00:48,547][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 23:00:53,545][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 23:00:57,221][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 23:01:01,276][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 23:01:11,802][WARN ][org.elasticsearch.action.index] [node_s3] unexpected error during the primary phase for action [indices:data/write/index], request [index {[test][type1][1], source[{"field1":"value1"}]}]
[test] IndexClosedException[closed]
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:170)
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:93)
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteSingleIndex(IndexNameExpressionResolver.java:206)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:414)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:130)
	at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:133)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:117)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:64)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:240)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$OperationTransportHandler.messageReceived(TransportReplicationAction.java:237)
	at org.elasticsearch.transport.local.LocalTransport.handleRequest(LocalTransport.java:290)
	at org.elasticsearch.transport.local.LocalTransport.messageReceived(LocalTransport.java:251)
	at org.elasticsearch.transport.local.LocalTransport$1.run(LocalTransport.java:229)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 23:01:14,173][WARN ][org.elasticsearch.test.transport] [node_s1] Transport response handler not found of id [75]
[2016-03-30 23:01:58,438][ERROR][org.elasticsearch.indices.analysis] [node_t0] exception while loading dictionary en_US
java.lang.IllegalStateException: failed to load hunspell dictionary for locale: en_US
	at org.elasticsearch.indices.analysis.HunspellService.lambda$new$92(HunspellService.java:87)
	at org.elasticsearch.indices.analysis.HunspellService$$Lambda$11/1617779494.apply(Unknown Source)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at org.elasticsearch.indices.analysis.HunspellService.getDictionary(HunspellService.java:104)
	at org.elasticsearch.indices.analysis.HunspellService.scanAndLoadDictionaries(HunspellService.java:128)
	at org.elasticsearch.indices.analysis.HunspellService.<init>(HunspellService.java:91)
	at org.elasticsearch.indices.analysis.AnalysisModule.configure(AnalysisModule.java:157)
	at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:59)
	at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:209)
	at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:79)
	at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:148)
	at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
	at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
	at org.elasticsearch.node.Node.<init>(Node.java:203)
	at org.elasticsearch.node.MockNode.<init>(MockNode.java:43)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:601)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:580)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1456)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1449)
	at org.elasticsearch.indices.analyze.HunspellServiceIT.testDicWithTwoAffs(HunspellServiceIT.java:101)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ElasticsearchException[Too many affix files exist for hunspell dictionary [en_US]]
	at org.elasticsearch.indices.analysis.HunspellService.loadDictionary(HunspellService.java:170)
	at org.elasticsearch.indices.analysis.HunspellService.lambda$new$92(HunspellService.java:85)
	... 58 more
[2016-03-30 23:02:17,051][WARN ][org.elasticsearch.snapshots] [node_s0] [dummy-repo][snap1] failed to restore snapshot
[bar*] IndexNotFoundException[no such index]
	at org.elasticsearch.snapshots.SnapshotUtils.filterIndices(SnapshotUtils.java:115)
	at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:157)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
	at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:160)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 23:02:23,460][WARN ][org.elasticsearch.snapshots] [node_s0] [dummy-repo][snap3] failed to restore snapshot
[baz*] IndexNotFoundException[no such index]
	at org.elasticsearch.snapshots.SnapshotUtils.filterIndices(SnapshotUtils.java:115)
	at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:157)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
	at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:160)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 23:02:44,446][WARN ][org.elasticsearch.snapshots] [node_s0] [dummy-repo][snap1] failed to restore snapshot
[test2] IndexNotFoundException[no such index]
	at org.elasticsearch.snapshots.SnapshotUtils.filterIndices(SnapshotUtils.java:79)
	at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:157)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
	at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:160)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 23:02:44,462][WARN ][org.elasticsearch.snapshots] [node_s0] [dummy-repo][snap1] failed to restore snapshot
[test2] IndexNotFoundException[no such index]
	at org.elasticsearch.snapshots.SnapshotUtils.filterIndices(SnapshotUtils.java:79)
	at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:157)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
	at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:160)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 23:03:07,850][WARN ][org.elasticsearch.cluster.metadata] [node_s0] [test] re-syncing mappings with cluster state because of types [[_default_]]
[2016-03-30 23:03:14,508][WARN ][org.elasticsearch.cluster.metadata] [node_s0] [test] re-syncing mappings with cluster state because of types [[_default_]]
[2016-03-30 23:03:16,286][WARN ][org.elasticsearch.cluster.metadata] [node_s0] [test] re-syncing mappings with cluster state because of types [[_default_]]
[2016-03-30 23:03:24,439][WARN ][org.elasticsearch.cluster.metadata] [node_s0] [test] re-syncing mappings with cluster state because of types [[_default_]]
[2016-03-30 23:03:25,143][WARN ][org.elasticsearch.cluster.metadata] [node_s0] [test] re-syncing mappings with cluster state because of types [[_default_]]
[2016-03-30 23:03:44,264][WARN ][org.elasticsearch.cluster.metadata] [node_s0] [test] re-syncing mappings with cluster state because of types [[_default_]]
