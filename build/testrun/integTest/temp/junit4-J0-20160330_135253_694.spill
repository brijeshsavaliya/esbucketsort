[2016-03-30 13:52:55,064][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=78,reason=Function not implemented
[2016-03-30 13:52:55,065][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.
[2016-03-30 13:53:23,384][ERROR][org.elasticsearch.indices.analysis] [node_t0] exception while loading dictionary en_US
java.lang.IllegalStateException: failed to load hunspell dictionary for locale: en_US
	at org.elasticsearch.indices.analysis.HunspellService.lambda$new$92(HunspellService.java:87)
	at org.elasticsearch.indices.analysis.HunspellService$$Lambda$11/791295053.apply(Unknown Source)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at org.elasticsearch.indices.analysis.HunspellService.getDictionary(HunspellService.java:104)
	at org.elasticsearch.indices.analysis.HunspellService.scanAndLoadDictionaries(HunspellService.java:128)
	at org.elasticsearch.indices.analysis.HunspellService.<init>(HunspellService.java:91)
	at org.elasticsearch.indices.analysis.AnalysisModule.configure(AnalysisModule.java:157)
	at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:59)
	at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:209)
	at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:79)
	at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:148)
	at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
	at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
	at org.elasticsearch.node.Node.<init>(Node.java:203)
	at org.elasticsearch.node.MockNode.<init>(MockNode.java:43)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:601)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:580)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1456)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1449)
	at org.elasticsearch.indices.analyze.HunspellServiceIT.testDicWithTwoAffs(HunspellServiceIT.java:101)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ElasticsearchException[Too many affix files exist for hunspell dictionary [en_US]]
	at org.elasticsearch.indices.analysis.HunspellService.loadDictionary(HunspellService.java:170)
	at org.elasticsearch.indices.analysis.HunspellService.lambda$new$92(HunspellService.java:85)
	... 58 more
[2016-03-30 13:53:23,953][ERROR][org.elasticsearch.indices.analysis] [node_t0] exception while loading dictionary en_US
java.lang.IllegalStateException: failed to load hunspell dictionary for locale: en_US
	at org.elasticsearch.indices.analysis.HunspellService.lambda$new$92(HunspellService.java:87)
	at org.elasticsearch.indices.analysis.HunspellService$$Lambda$11/791295053.apply(Unknown Source)
	at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)
	at org.elasticsearch.indices.analysis.HunspellService.getDictionary(HunspellService.java:104)
	at org.elasticsearch.indices.analysis.HunspellService.scanAndLoadDictionaries(HunspellService.java:128)
	at org.elasticsearch.indices.analysis.HunspellService.<init>(HunspellService.java:91)
	at org.elasticsearch.indices.analysis.AnalysisModule.configure(AnalysisModule.java:157)
	at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:59)
	at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:209)
	at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:79)
	at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:148)
	at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
	at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
	at org.elasticsearch.node.Node.<init>(Node.java:203)
	at org.elasticsearch.node.MockNode.<init>(MockNode.java:43)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:601)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:580)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1456)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1449)
	at org.elasticsearch.indices.analyze.HunspellServiceIT.testDicWithNoAff(HunspellServiceIT.java:83)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ElasticsearchException[Missing affix file for hunspell dictionary [en_US]]
	at org.elasticsearch.indices.analysis.HunspellService.loadDictionary(HunspellService.java:167)
	at org.elasticsearch.indices.analysis.HunspellService.lambda$new$92(HunspellService.java:85)
	... 58 more
[2016-03-30 13:54:06,739][WARN ][org.elasticsearch.discovery.zen] [node_t4] master left (reason = shut_down), current nodes: {{node_t2}{87WoS79CSKy-40LADMyyBA}{127.0.0.1}{127.0.0.1:30102}[mode=>network],{node_t4}{isyxPRHlTQqkeclxGz_OGA}{127.0.0.1}{127.0.0.1:30104}[mode=>network],{node_t3}{w_p-4rCgQVSOKohrvIliDg}{127.0.0.1}{127.0.0.1:30103}[mode=>network],}
[2016-03-30 13:54:06,740][WARN ][org.elasticsearch.discovery.zen] [node_t2] master left (reason = shut_down), current nodes: {{node_t2}{87WoS79CSKy-40LADMyyBA}{127.0.0.1}{127.0.0.1:30102}[mode=>network],{node_t4}{isyxPRHlTQqkeclxGz_OGA}{127.0.0.1}{127.0.0.1:30104}[mode=>network],{node_t3}{w_p-4rCgQVSOKohrvIliDg}{127.0.0.1}{127.0.0.1:30103}[mode=>network],}
[2016-03-30 13:54:06,739][WARN ][org.elasticsearch.discovery.zen] [node_t3] master left (reason = shut_down), current nodes: {{node_t2}{87WoS79CSKy-40LADMyyBA}{127.0.0.1}{127.0.0.1:30102}[mode=>network],{node_t4}{isyxPRHlTQqkeclxGz_OGA}{127.0.0.1}{127.0.0.1:30104}[mode=>network],{node_t3}{w_p-4rCgQVSOKohrvIliDg}{127.0.0.1}{127.0.0.1:30103}[mode=>network],}
[2016-03-30 13:54:13,687][WARN ][org.elasticsearch.discovery.zen] [node_t1] master left (reason = shut_down), current nodes: {{node_t2}{Ur2_xptvR_ujTHBrpQf78g}{127.0.0.1}{127.0.0.1:30107}[mode=>network],{node_t1}{peleQYQQRHm6iDe23aBUtQ}{127.0.0.1}{127.0.0.1:30106}[mode=>network],}
[2016-03-30 13:54:13,688][WARN ][org.elasticsearch.discovery.zen] [node_t2] master left (reason = shut_down), current nodes: {{node_t2}{Ur2_xptvR_ujTHBrpQf78g}{127.0.0.1}{127.0.0.1:30107}[mode=>network],{node_t1}{peleQYQQRHm6iDe23aBUtQ}{127.0.0.1}{127.0.0.1:30106}[mode=>network],}
[2016-03-30 13:54:22,792][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:22,800][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:22,939][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:23,068][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:23,257][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:23,658][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:23,742][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:23,872][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,024][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,090][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,167][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,235][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,263][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,306][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,528][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,545][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,566][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,581][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,599][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,614][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,636][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,651][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,669][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,687][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,707][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,727][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,748][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,768][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,789][WARN ][org.elasticsearch.gateway] [node_s1] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:24,806][WARN ][org.elasticsearch.gateway] [node_s0] [idx]: failed to write index state
java.lang.NullPointerException
	at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:145)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:257)
	at org.elasticsearch.search.warmer.IndexWarmersMetaData.toXContent(IndexWarmersMetaData.java:244)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.toXContent(IndexMetaData.java:832)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:182)
	at org.elasticsearch.gateway.MetaStateService$2.toXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.write(MetaDataStateFormat.java:120)
	at org.elasticsearch.gateway.MetaStateService.writeIndex(MetaStateService.java:135)
	at org.elasticsearch.gateway.GatewayMetaState.clusterChanged(GatewayMetaState.java:163)
	at org.elasticsearch.gateway.Gateway.clusterChanged(Gateway.java:185)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:54:39,940][TRACE][org.elasticsearch.action.admin.cluster.health] [node_t0] observer: sampled state rejected by predicate (version [4], status [BEING_APPLIED]). adding listener to ClusterService
[2016-03-30 13:54:39,941][TRACE][org.elasticsearch.action.admin.cluster.health] [node_t0] Calculating health based on state version [4]
[2016-03-30 13:54:39,942][TRACE][org.elasticsearch.action.admin.cluster.health] [node_t0] observer: post adding listener: accepting current cluster state (version [4], status [APPLIED])
[2016-03-30 13:54:39,943][TRACE][org.elasticsearch.action.admin.cluster.health] [node_t0] Calculating health based on state version [4]
[2016-03-30 13:54:40,033][TRACE][org.elasticsearch.action.admin.cluster.health] [node_t0] Calculating health based on state version [6]
[2016-03-30 13:54:40,034][TRACE][org.elasticsearch.action.admin.cluster.health] [node_t0] Calculating health based on state version [6]
[2016-03-30 13:54:40,034][TRACE][org.elasticsearch.action.admin.cluster.health] [node_t0] Calculating health based on state version [6]
[2016-03-30 13:54:43,384][WARN ][org.elasticsearch.discovery.zen.publish] [node_t1] received a cluster state from a different master than the current one, rejecting (received {abc}{local}{local[abc]}, current {node_t0}{0Jg899l5SWuvFhuytHIdzg}{127.0.0.1}{127.0.0.1:9480}[mode=>network])
[2016-03-30 13:54:43,426][WARN ][org.elasticsearch.discovery.zen] [node_t1] master left (reason = shut_down), current nodes: {{node_t1}{b7brEKXYRcGhJVXIm11oOA}{127.0.0.1}{127.0.0.1:9481}[mode=>network],}
[2016-03-30 13:54:56,851][WARN ][org.elasticsearch.discovery.zen] [node_t0] master left (reason = shut_down), current nodes: {{node_t2}{vlm0-L4HRf28TLxcRogMEg}{127.0.0.1}{127.0.0.1:9442}[master=>false, mode=>network],{node_t3}{GUCyWBUkRoy23CrtjZ6rgA}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],{node_t0}{sVQFmLK3RsiiI4MtJvhszQ}{127.0.0.1}{127.0.0.1:9440}[mode=>network, data=>false],}
[2016-03-30 13:54:56,862][WARN ][org.elasticsearch.discovery.zen] [node_t2] master left (reason = transport disconnected), current nodes: {{node_t2}{vlm0-L4HRf28TLxcRogMEg}{127.0.0.1}{127.0.0.1:9442}[master=>false, mode=>network],{node_t3}{GUCyWBUkRoy23CrtjZ6rgA}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],{node_t0}{sVQFmLK3RsiiI4MtJvhszQ}{127.0.0.1}{127.0.0.1:9440}[data=>false, mode=>network],}
[2016-03-30 13:54:56,862][WARN ][org.elasticsearch.discovery.zen] [node_t3] master left (reason = transport disconnected), current nodes: {{node_t2}{vlm0-L4HRf28TLxcRogMEg}{127.0.0.1}{127.0.0.1:9442}[mode=>network, master=>false],{node_t3}{GUCyWBUkRoy23CrtjZ6rgA}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],{node_t0}{sVQFmLK3RsiiI4MtJvhszQ}{127.0.0.1}{127.0.0.1:9440}[data=>false, mode=>network],}
[2016-03-30 13:55:00,134][WARN ][org.elasticsearch.discovery.zen] [node_t2] master left (reason = transport disconnected), current nodes: {{node_t2}{vlm0-L4HRf28TLxcRogMEg}{127.0.0.1}{127.0.0.1:9442}[master=>false, mode=>network],{node_t3}{GUCyWBUkRoy23CrtjZ6rgA}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],}
[2016-03-30 13:55:00,134][WARN ][org.elasticsearch.discovery.zen] [node_t3] master left (reason = transport disconnected), current nodes: {{node_t2}{vlm0-L4HRf28TLxcRogMEg}{127.0.0.1}{127.0.0.1:9442}[master=>false, mode=>network],{node_t3}{GUCyWBUkRoy23CrtjZ6rgA}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],}
[2016-03-30 13:55:03,351][WARN ][org.elasticsearch.discovery.zen.elect] ignoring master [{_node_id}{local}{local[_id]}], because the version [1.6.0] is lower than the minimum compatible version [2.0.0-beta1]
[2016-03-30 13:55:11,997][WARN ][org.elasticsearch.cluster] [node_t0] Failed to execute NodeStatsAction for ClusterInfoUpdateJob
ElasticsearchException[force exception on [cluster:monitor/nodes/stats]]
	at org.elasticsearch.cluster.ClusterInfoServiceIT$BlockingActionFilter.apply(ClusterInfoServiceIT.java:105)
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:63)
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:99)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:77)
	at org.elasticsearch.cluster.InternalClusterInfoService.updateNodeStats(InternalClusterInfoService.java:271)
	at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:307)
	at org.elasticsearch.cluster.ClusterInfoServiceIT.testClusterInfoServiceInformationClearOnError(ClusterInfoServiceIT.java:250)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:55:11,999][WARN ][org.elasticsearch.cluster] [node_t0] Failed to execute IndicesStatsAction for ClusterInfoUpdateJob
ElasticsearchException[force exception on [indices:monitor/stats]]
	at org.elasticsearch.cluster.ClusterInfoServiceIT$BlockingActionFilter.apply(ClusterInfoServiceIT.java:105)
	at org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:63)
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:99)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:77)
	at org.elasticsearch.cluster.InternalClusterInfoService.updateIndicesStats(InternalClusterInfoService.java:285)
	at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:336)
	at org.elasticsearch.cluster.ClusterInfoServiceIT.testClusterInfoServiceInformationClearOnError(ClusterInfoServiceIT.java:250)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:55:20,107][WARN ][org.elasticsearch.discovery.zen] [node_t0] master left (reason = transport disconnected), current nodes: {{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],{node_t0}{YrYchOe4TZaF8_LfBjo5vw}{127.0.0.1}{127.0.0.1:9440}[master=>true, mode=>network, data=>false],{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[mode=>network, data=>false, master=>true],}
[2016-03-30 13:55:20,109][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t0] failed to send ping to [{node_t1}{B8ueoxDfQ6q-fh1-89oQ2w}{127.0.0.1}{127.0.0.1:9441}[data=>false, master=>true, mode=>network]]
SendRequestTransportException[[node_t1][127.0.0.1:9441][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t1][127.0.0.1:9441] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:857)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:368)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t1][127.0.0.1:9441] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 12 more
[2016-03-30 13:55:20,114][WARN ][org.elasticsearch.cluster.service] [node_t1] failing [zen-disco-node_failed({node_t0}{YrYchOe4TZaF8_LfBjo5vw}{127.0.0.1}{127.0.0.1:9440}[master=>true, data=>false, mode=>network]), reason transport disconnected]: failed to commit cluster state version [8]
FailedToCommitClusterStateException[failed to get enough masters to ack sent cluster state. [1] left]
	at org.elasticsearch.discovery.zen.publish.PublishClusterStateAction$SendingController.waitForCommit(PublishClusterStateAction.java:509)
	at org.elasticsearch.discovery.zen.publish.PublishClusterStateAction.innerPublish(PublishClusterStateAction.java:173)
	at org.elasticsearch.discovery.zen.publish.PublishClusterStateAction.publish(PublishClusterStateAction.java:138)
	at org.elasticsearch.discovery.zen.ZenDiscovery.publish(ZenDiscovery.java:319)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.elasticsearch.common.inject.internal.ConstructionContext$DelegatingInvocationHandler.invoke(ConstructionContext.java:111)
	at com.sun.proxy.$Proxy37.publish(Unknown Source)
	at org.elasticsearch.discovery.DiscoveryService.publish(DiscoveryService.java:130)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:513)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:55:20,117][ERROR][org.elasticsearch.discovery.zen] [node_t1] unexpected failure during [zen-disco-node_failed({node_t0}{YrYchOe4TZaF8_LfBjo5vw}{127.0.0.1}{127.0.0.1:9440}[master=>true, data=>false, mode=>network]), reason transport disconnected]
FailedToCommitClusterStateException[failed to get enough masters to ack sent cluster state. [1] left]
	at org.elasticsearch.discovery.zen.publish.PublishClusterStateAction$SendingController.waitForCommit(PublishClusterStateAction.java:509)
	at org.elasticsearch.discovery.zen.publish.PublishClusterStateAction.innerPublish(PublishClusterStateAction.java:173)
	at org.elasticsearch.discovery.zen.publish.PublishClusterStateAction.publish(PublishClusterStateAction.java:138)
	at org.elasticsearch.discovery.zen.ZenDiscovery.publish(ZenDiscovery.java:319)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.elasticsearch.common.inject.internal.ConstructionContext$DelegatingInvocationHandler.invoke(ConstructionContext.java:111)
	at com.sun.proxy.$Proxy37.publish(Unknown Source)
	at org.elasticsearch.discovery.DiscoveryService.publish(DiscoveryService.java:130)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:513)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:55:20,118][WARN ][org.elasticsearch.discovery.zen] [node_t1] failed to publish to min_master_nodes, current nodes: {{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[mode=>network, master=>false],{node_t1}{B8ueoxDfQ6q-fh1-89oQ2w}{127.0.0.1}{127.0.0.1:9441}[master=>true, mode=>network, data=>false],{node_t0}{YrYchOe4TZaF8_LfBjo5vw}{127.0.0.1}{127.0.0.1:9440}[master=>true, data=>false, mode=>network],{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[mode=>network, master=>true, data=>false],}
[2016-03-30 13:55:20,119][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9440}]
SendRequestTransportException[[node_t0][127.0.0.1:9440][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:857)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:368)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 12 more
[2016-03-30 13:55:20,122][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_4#}{127.0.0.1}{127.0.0.1:9443}]
SendRequestTransportException[[node_t3][127.0.0.1:9443][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:857)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:368)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 12 more
[2016-03-30 13:55:20,126][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[master=>true, mode=>network, data=>false]]
SendRequestTransportException[[node_t2][127.0.0.1:9442][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:857)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:368)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 12 more
[2016-03-30 13:55:20,122][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_3#}{127.0.0.1}{127.0.0.1:9442}]
SendRequestTransportException[[node_t2][127.0.0.1:9442][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:857)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:368)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 12 more
[2016-03-30 13:55:20,126][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[mode=>network, master=>false]]
SendRequestTransportException[[node_t3][127.0.0.1:9443][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:857)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:368)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 12 more
[2016-03-30 13:55:20,125][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{node_t0}{YrYchOe4TZaF8_LfBjo5vw}{127.0.0.1}{127.0.0.1:9440}[mode=>network, master=>true, data=>false]]
SendRequestTransportException[[node_t0][127.0.0.1:9440][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:857)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:368)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 12 more
[2016-03-30 13:55:20,407][WARN ][org.elasticsearch.discovery.zen] [node_t2] master left (reason = transport disconnected), current nodes: {{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],{node_t0}{YrYchOe4TZaF8_LfBjo5vw}{127.0.0.1}{127.0.0.1:9440}[mode=>network, master=>true, data=>false],{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[master=>true, mode=>network, data=>false],}
[2016-03-30 13:55:20,408][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t2] failed to send ping to [{node_t1}{B8ueoxDfQ6q-fh1-89oQ2w}{127.0.0.1}{127.0.0.1:9441}[mode=>network, master=>true, data=>false]]
SendRequestTransportException[[node_t1][127.0.0.1:9441][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t1][127.0.0.1:9441] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
	at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
	at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:857)
	at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:368)
	at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
	at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t1][127.0.0.1:9441] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 12 more
[2016-03-30 13:55:20,529][WARN ][org.elasticsearch.discovery.zen] [node_t3] master left (reason = transport disconnected), current nodes: {{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],{node_t0}{YrYchOe4TZaF8_LfBjo5vw}{127.0.0.1}{127.0.0.1:9440}[data=>false, mode=>network, master=>true],{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[data=>false, mode=>network, master=>true],}
[2016-03-30 13:55:21,627][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9440}]
SendRequestTransportException[[node_t0][127.0.0.1:9440][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2.doRun(UnicastZenPing.java:249)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:21,629][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_4#}{127.0.0.1}{127.0.0.1:9443}]
SendRequestTransportException[[node_t3][127.0.0.1:9443][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2.doRun(UnicastZenPing.java:249)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:21,627][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_3#}{127.0.0.1}{127.0.0.1:9442}]
SendRequestTransportException[[node_t2][127.0.0.1:9442][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2.doRun(UnicastZenPing.java:249)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:21,632][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[mode=>network, master=>false]]
SendRequestTransportException[[node_t3][127.0.0.1:9443][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2.doRun(UnicastZenPing.java:249)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:21,631][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[mode=>network, data=>false, master=>true]]
SendRequestTransportException[[node_t2][127.0.0.1:9442][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2.doRun(UnicastZenPing.java:249)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:21,629][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{node_t0}{YrYchOe4TZaF8_LfBjo5vw}{127.0.0.1}{127.0.0.1:9440}[master=>true, mode=>network, data=>false]]
SendRequestTransportException[[node_t0][127.0.0.1:9440][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2.doRun(UnicastZenPing.java:249)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:23,135][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9440}]
SendRequestTransportException[[node_t0][127.0.0.1:9440][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2$1.doRun(UnicastZenPing.java:253)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:23,138][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network]]
SendRequestTransportException[[node_t3][127.0.0.1:9443][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2$1.doRun(UnicastZenPing.java:253)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:23,138][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[mode=>network, master=>true, data=>false]]
SendRequestTransportException[[node_t2][127.0.0.1:9442][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2$1.doRun(UnicastZenPing.java:253)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:23,138][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{node_t0}{YrYchOe4TZaF8_LfBjo5vw}{127.0.0.1}{127.0.0.1:9440}[master=>true, data=>false, mode=>network]]
SendRequestTransportException[[node_t0][127.0.0.1:9440][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2$1.doRun(UnicastZenPing.java:253)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:23,136][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_4#}{127.0.0.1}{127.0.0.1:9443}]
SendRequestTransportException[[node_t3][127.0.0.1:9443][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2$1.doRun(UnicastZenPing.java:253)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t3][127.0.0.1:9443] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:23,135][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_3#}{127.0.0.1}{127.0.0.1:9442}]
SendRequestTransportException[[node_t2][127.0.0.1:9442][internal:discovery/zen/unicast]]; nested: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPingRequestToNode(UnicastZenPing.java:440)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:426)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$2$1.doRun(UnicastZenPing.java:253)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t2][127.0.0.1:9442] DISCONNECT: simulated]
	at org.elasticsearch.test.transport.MockTransportService$1.sendRequest(MockTransportService.java:161)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 7 more
[2016-03-30 13:55:31,549][WARN ][org.elasticsearch.discovery.zen] [node_t1] master left (reason = shut_down), current nodes: {{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],{node_t1}{B8ueoxDfQ6q-fh1-89oQ2w}{127.0.0.1}{127.0.0.1:9441}[master=>true, mode=>network, data=>false],{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[mode=>network, master=>true, data=>false],}
[2016-03-30 13:55:31,549][WARN ][org.elasticsearch.discovery.zen] [node_t2] master left (reason = shut_down), current nodes: {{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],{node_t1}{B8ueoxDfQ6q-fh1-89oQ2w}{127.0.0.1}{127.0.0.1:9441}[data=>false, mode=>network, master=>true],{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[master=>true, mode=>network, data=>false],}
[2016-03-30 13:55:31,552][WARN ][org.elasticsearch.discovery.zen] [node_t3] master left (reason = transport disconnected), current nodes: {{node_t3}{Ru8eiy-0TzGAWxjWP1LF2g}{127.0.0.1}{127.0.0.1:9443}[master=>false, mode=>network],{node_t1}{B8ueoxDfQ6q-fh1-89oQ2w}{127.0.0.1}{127.0.0.1:9441}[master=>true, mode=>network, data=>false],{node_t2}{nd6j0cc_T1Wr2e53LO9gBA}{127.0.0.1}{127.0.0.1:9442}[data=>false, master=>true, mode=>network],}
[2016-03-30 13:55:42,356][WARN ][org.elasticsearch.discovery.zen] [node_t1] master left (reason = shut_down), current nodes: {{node_t1}{9zHCDCjGQKip-aIkgvNT5g}{127.0.0.1}{127.0.0.1:9481}[mode=>network],}
[2016-03-30 13:56:10,702][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] wipe translog location - creating new translog
[2016-03-30 13:56:11,024][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 13:56:11,361][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=10551, numOps=142, translogFileGeneration= 1}
[2016-03-30 13:56:11,435][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 13:56:11,446][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:11,459][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[kuLYOiLVS9eaPqpZK3T7ZA], [P], v[3], s[INITIALIZING], a[id=OURq8Tc8TimyMBQWWuK4Kg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-03-30T20:56:11.247Z]], indexUUID [1Wax1szhQfKV5CFyGKO5Qg], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:11,572][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 2}
[2016-03-30 13:56:11,573][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 2}
[2016-03-30 13:56:11,646][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 13:56:11,657][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:11,659][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[kuLYOiLVS9eaPqpZK3T7ZA], [P], v[3], s[INITIALIZING], a[id=5YH0zkU0Qvm5vCNn5LuHQg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-30T20:56:11.461Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]], indexUUID [1Wax1szhQfKV5CFyGKO5Qg], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:11,765][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 3}
[2016-03-30 13:56:11,766][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 3}
[2016-03-30 13:56:11,767][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 3}
[2016-03-30 13:56:11,839][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 13:56:11,850][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:11,852][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[kuLYOiLVS9eaPqpZK3T7ZA], [P], v[3], s[INITIALIZING], a[id=dpxS8kZ4QcWBxEeI9usSlw], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-30T20:56:11.660Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]], indexUUID [1Wax1szhQfKV5CFyGKO5Qg], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:11,954][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 4}
[2016-03-30 13:56:11,955][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 4}
[2016-03-30 13:56:11,956][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 4}
[2016-03-30 13:56:11,956][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 4}
[2016-03-30 13:56:11,997][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 13:56:11,999][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:12,001][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[kuLYOiLVS9eaPqpZK3T7ZA], [P], v[3], s[INITIALIZING], a[id=G6xIABy9RN2sEbW7DQhIDA], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-30T20:56:11.853Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]], indexUUID [1Wax1szhQfKV5CFyGKO5Qg], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:12,090][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 5}
[2016-03-30 13:56:12,091][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 5}
[2016-03-30 13:56:12,091][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 5}
[2016-03-30 13:56:12,092][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 5}
[2016-03-30 13:56:12,093][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 5}
[2016-03-30 13:56:12,129][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 13:56:12,140][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:12,141][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[kuLYOiLVS9eaPqpZK3T7ZA], [P], v[3], s[INITIALIZING], a[id=nL6IDT4NRGOCeV9Qki1fYw], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-30T20:56:12.002Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]], indexUUID [1Wax1szhQfKV5CFyGKO5Qg], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:12,186][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] open uncommitted translog checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 6}
[2016-03-30 13:56:12,187][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 6}
[2016-03-30 13:56:12,188][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 6}
[2016-03-30 13:56:12,188][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 6}
[2016-03-30 13:56:12,189][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 6}
[2016-03-30 13:56:12,190][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] recovered local translog from checkpoint Checkpoint{offset=413, numOps=5, translogFileGeneration= 6}
[2016-03-30 13:56:12,252][DEBUG][org.elasticsearch.index.translog] [node_s0] [test][0] translog closed
[2016-03-30 13:56:12,254][WARN ][org.elasticsearch.indices.cluster] [node_s0] [[test][0]] marking and sending shard failed due to [failed recovery]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:12,256][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[kuLYOiLVS9eaPqpZK3T7ZA], [P], v[3], s[INITIALIZING], a[id=RL1_vKMyQ--zjAk58ODfqA], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-30T20:56:12.143Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]], indexUUID [1Wax1szhQfKV5CFyGKO5Qg], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]
[test][[test][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:234)
	at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$122(StoreRecovery.java:77)
	at org.elasticsearch.index.shard.StoreRecovery$$Lambda$487/1536812624.run(Unknown Source)
	at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:122)
	at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:75)
	at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1104)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$127(IndicesClusterStateService.java:637)
	at org.elasticsearch.indices.cluster.IndicesClusterStateService$$Lambda$486/1631602532.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:156)
	at org.elasticsearch.test.engine.MockInternalEngine.<init>(MockInternalEngine.java:36)
	at org.elasticsearch.test.engine.MockEngineFactory.newReadWriteEngine(MockEngineFactory.java:45)
	at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1435)
	at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1430)
	at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:860)
	at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:838)
	at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:230)
	... 10 more
Caused by: [test][[test][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:231)
	at org.elasticsearch.index.engine.InternalEngine.<init>(InternalEngine.java:153)
	... 17 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb];
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1227)
	at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
	at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
	at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
	at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:217)
	... 18 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]
	at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1185)
	at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1217)
	... 23 more
[2016-03-30 13:56:12,257][WARN ][org.elasticsearch.cluster.action.shard] [node_s0] [test][0] received shard failed for [test][0], node[kuLYOiLVS9eaPqpZK3T7ZA], [P], v[3], s[INITIALIZING], a[id=RL1_vKMyQ--zjAk58ODfqA], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-30T20:56:12.143Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x10a871b, got: 0x64c72ddb]; ]], indexUUID [1Wax1szhQfKV5CFyGKO5Qg], message [master {node_s0}{kuLYOiLVS9eaPqpZK3T7ZA}{local}{local[87]}[mode=>local] marked shard as initializing, but shard is marked as failed, resend shard failure], failure [Unknown]
[2016-03-30 13:56:57,055][WARN ][org.elasticsearch.action.index] [node_t1] [testidx][0] failed to perform indices:data/write/index[r] on node {node_t0}{b459ArHSSg6KdMubh6sZ5g}{127.0.0.1}{127.0.0.1:9440}[mode=>network]
SendRequestTransportException[[node_t0][127.0.0.1:9440][indices:data/write/index[r]]]; nested: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: prevented indices:data/write/index[r] request];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicationPhase.performOnReplica(TransportReplicationAction.java:856)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicationPhase.doRun(TransportReplicationAction.java:831)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.finishAndMoveToReplication(TransportReplicationAction.java:665)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:609)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:265)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:262)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: prevented indices:data/write/index[r] request]
	at org.elasticsearch.test.transport.MockTransportService$2.sendRequest(MockTransportService.java:209)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 13 more
[2016-03-30 13:56:57,072][WARN ][org.elasticsearch.cluster.action.shard] [node_t0] [testidx][0] received shard failed for [testidx][0], node[b459ArHSSg6KdMubh6sZ5g], [R], v[3], s[STARTED], a[id=eHcmI7CPTS2yx3D4n7jpDQ], indexUUID [cPFAUcQhTHSrn04tQmPaGQ], message [failed to perform indices:data/write/index[r] on replica on node {node_t0}{b459ArHSSg6KdMubh6sZ5g}{127.0.0.1}{127.0.0.1:9440}[mode=>network]], failure [SendRequestTransportException[[node_t0][127.0.0.1:9440][indices:data/write/index[r]]]; nested: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: prevented indices:data/write/index[r] request]; ]
SendRequestTransportException[[node_t0][127.0.0.1:9440][indices:data/write/index[r]]]; nested: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: prevented indices:data/write/index[r] request];
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:323)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicationPhase.performOnReplica(TransportReplicationAction.java:856)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicationPhase.doRun(TransportReplicationAction.java:831)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.finishAndMoveToReplication(TransportReplicationAction.java:665)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:609)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:265)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:262)
	at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: ConnectTransportException[[node_t0][127.0.0.1:9440] DISCONNECT: prevented indices:data/write/index[r] request]
	at org.elasticsearch.test.transport.MockTransportService$2.sendRequest(MockTransportService.java:209)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.sendRequest(MockTransportService.java:422)
	at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:312)
	... 13 more
[2016-03-30 13:56:57,278][WARN ][org.elasticsearch.discovery.zen] [node_t1] master left (reason = shut_down), current nodes: {{node_t1}{l1UzgfUMR_G-aX4bQskxGw}{127.0.0.1}{127.0.0.1:9441}[mode=>network],}
[2016-03-30 13:57:26,115][WARN ][org.elasticsearch.test.transport] [node_s1] Transport response handler not found of id [590]
[2016-03-30 13:58:32,314][WARN ][org.elasticsearch.snapshots] [node_s0] [dummy-repo][snap1] failed to restore snapshot
[test2] IndexNotFoundException[no such index]
	at org.elasticsearch.snapshots.SnapshotUtils.filterIndices(SnapshotUtils.java:79)
	at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:157)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
	at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:160)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:58:32,324][WARN ][org.elasticsearch.snapshots] [node_s0] [dummy-repo][snap1] failed to restore snapshot
[test2] IndexNotFoundException[no such index]
	at org.elasticsearch.snapshots.SnapshotUtils.filterIndices(SnapshotUtils.java:79)
	at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:157)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
	at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:160)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:58:59,219][WARN ][org.elasticsearch.snapshots] [node_s0] [dummy-repo][snap1] failed to restore snapshot
[bar*] IndexNotFoundException[no such index]
	at org.elasticsearch.snapshots.SnapshotUtils.filterIndices(SnapshotUtils.java:115)
	at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:157)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
	at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:160)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:59:05,222][WARN ][org.elasticsearch.snapshots] [node_s0] [dummy-repo][snap3] failed to restore snapshot
[baz*] IndexNotFoundException[no such index]
	at org.elasticsearch.snapshots.SnapshotUtils.filterIndices(SnapshotUtils.java:115)
	at org.elasticsearch.snapshots.RestoreService.restoreSnapshot(RestoreService.java:157)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:82)
	at org.elasticsearch.action.admin.cluster.snapshots.restore.TransportRestoreSnapshotAction.masterOperation(TransportRestoreSnapshotAction.java:41)
	at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$3.doRun(TransportMasterNodeAction.java:160)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:59:28,678][WARN ][org.elasticsearch.bwcompat] Old repositories tests contain extra repo: 2.0.0-beta1
[2016-03-30 13:59:29,196][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,197][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,198][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.7.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.7.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.7.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,230][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,230][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,230][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,264][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,264][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,264][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.1.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.1.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.1.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,293][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,293][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,293][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.6.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.6.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.6.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,335][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,335][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,335][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,366][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,366][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,366][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.7.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.7.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.7.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,411][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,411][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,412][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,453][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,453][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,453][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,488][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,488][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,488][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.5] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.5] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.5] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,536][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,536][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,536][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,567][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,568][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,602][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,602][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,602][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.5.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.5.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.5.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,635][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,635][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,664][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,664][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,664][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,698][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,698][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,698][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.0.beta1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.0.beta1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.0.beta1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,730][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,730][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,730][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.4] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,762][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,762][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,762][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.7] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.7] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.7] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,812][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,812][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,844][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,844][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,845][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,875][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,875][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,876][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.4] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,906][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,907][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,907][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.4] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.4] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,938][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,938][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,939][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.1.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.1.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.1.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:29,969][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:29,970][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:29,970][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.4.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.4.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.4.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,009][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,009][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,010][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.5] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.5] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.5] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,042][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,042][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,042][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.6.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.6.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.6.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,078][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,078][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,078][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.5.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.5.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.5.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,120][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,121][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.0.rc2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.0.rc2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.0.rc2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,150][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,150][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.0.beta2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.0.beta2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.0.beta2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,183][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,183][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,256][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,256][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,257][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.7.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.7.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.7.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,292][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,292][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,293][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,325][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,326][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,328][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,375][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,376][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,376][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.6.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.6.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.6.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,417][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,417][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,418][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.5.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.5.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.5.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,468][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,469][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,469][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.6] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.6] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.6] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,517][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,517][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,517][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,552][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,552][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,552][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.7.3] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.7.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.7.3] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,597][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,597][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,597][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.9] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.9] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.9] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,632][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,632][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,632][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.1.0] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.1.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.1.0] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,667][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,667][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,667][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.2.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.2.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.2.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,707][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,708][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.0.0.rc1] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.0.0.rc1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.0.0.rc1] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,739][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-0.20.6-and-1.1.2] because it cannot be upgraded]; nested: IllegalStateException[The index [index-0.20.6-and-1.1.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-0.20.6-and-1.1.2] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:30,766][WARN ][org.elasticsearch.snapshots] [node_t0] time cluster setting [discovery.zen.publish_timeout] with value [30000] is missing units; assuming default units (ms) but in future versions this will be a hard error
[2016-03-30 13:59:30,766][WARN ][org.elasticsearch.snapshots] [node_t0] byte-sized cluster setting [indices.recovery.file_chunk_size] with value [524288] is missing units; assuming default units (b) but in future versions this will be a hard error
[2016-03-30 13:59:30,766][WARN ][org.elasticsearch.snapshots] [node_t0] [test_repo:test_1] failed to restore snapshot
SnapshotRestoreException[[test_repo:test_1] cannot restore index [index-1.3.8] because it cannot be upgraded]; nested: IllegalStateException[The index [index-1.3.8] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.];
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:209)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: The index [index-1.3.8] was created before v2.0.0.beta1 and wasn't upgraded. This index should be open using a version before 3.0.0-SNAPSHOT and upgraded using the upgrade API.
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkSupportedVersion(MetaDataIndexUpgradeService.java:96)
	at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:72)
	at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:207)
	... 8 more
[2016-03-30 13:59:58,527][WARN ][org.elasticsearch.action.index] [node_s2] unexpected error during the primary phase for action [indices:data/write/index], request [index {[test][type1][1], source[{"field1":"value1"}]}]
[test] IndexClosedException[closed]
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:170)
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:93)
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteSingleIndex(IndexNameExpressionResolver.java:206)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:414)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:130)
	at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:133)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:117)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:64)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:59)
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:337)
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:337)
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:86)
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:60)
	at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:68)
	at org.elasticsearch.indices.state.SimpleIndexStateIT.testSimpleOpenClose(SimpleIndexStateIT.java:77)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 13:59:59,936][WARN ][org.elasticsearch.test.transport] [node_s1] Transport response handler not found of id [25]
[2016-03-30 14:00:25,589][WARN ][org.elasticsearch.action.index] [node_t1] unexpected error during the primary phase for action [indices:data/write/index], request [index {[test][type1][1], source[{"field1":"value1"}]}]
[test] IndexClosedException[closed]
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:170)
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:93)
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteSingleIndex(IndexNameExpressionResolver.java:206)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:414)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:130)
	at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:133)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:117)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:64)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:59)
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:337)
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:337)
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:86)
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:60)
	at org.elasticsearch.gateway.GatewayIndexStateIT.testSimpleOpenClose(GatewayIndexStateIT.java:111)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:00:28,445][WARN ][org.elasticsearch.action.index] [node_t1] unexpected error during the primary phase for action [indices:data/write/index], request [index {[test][type1][1], source[{"field1":"value1"}]}]
[test] IndexClosedException[closed]
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:170)
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:93)
	at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteSingleIndex(IndexNameExpressionResolver.java:206)
	at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:414)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:130)
	at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:133)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:117)
	at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:64)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:59)
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:337)
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:337)
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:86)
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:60)
	at org.elasticsearch.gateway.GatewayIndexStateIT.testSimpleOpenClose(GatewayIndexStateIT.java:154)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:00:38,526][WARN ][org.elasticsearch.gateway] [node_t0] ignoring dangled index [test] on node [{node_t2}{NJNhcZmXTN6cP3_rl5QeJA}{local}{local[244]}[mode=>local]] due to an existing alias with the same name
[2016-03-30 14:00:38,527][WARN ][org.elasticsearch.gateway] [node_t0] ignoring dangled index [test] on node [{node_t2}{NJNhcZmXTN6cP3_rl5QeJA}{local}{local[244]}[mode=>local]] due to an existing alias with the same name
[2016-03-30 14:00:38,818][WARN ][org.elasticsearch.gateway] [node_t0] ignoring dangled index [test] on node [{node_t2}{NJNhcZmXTN6cP3_rl5QeJA}{local}{local[244]}[mode=>local]] due to an existing alias with the same name
[2016-03-30 14:00:38,818][WARN ][org.elasticsearch.gateway] [node_t0] ignoring dangled index [test] on node [{node_t2}{NJNhcZmXTN6cP3_rl5QeJA}{local}{local[244]}[mode=>local]] due to an existing alias with the same name
[2016-03-30 14:00:38,972][WARN ][org.elasticsearch.gateway] [node_t0] ignoring dangled index [test] on node [{node_t2}{NJNhcZmXTN6cP3_rl5QeJA}{local}{local[244]}[mode=>local]] due to an existing alias with the same name
[2016-03-30 14:00:39,111][WARN ][org.elasticsearch.gateway] [node_t0] ignoring dangled index [test] on node [{node_t2}{NJNhcZmXTN6cP3_rl5QeJA}{local}{local[244]}[mode=>local]] due to an existing alias with the same name
[2016-03-30 14:00:39,111][WARN ][org.elasticsearch.gateway] [node_t0] ignoring dangled index [test] on node [{node_t2}{NJNhcZmXTN6cP3_rl5QeJA}{local}{local[244]}[mode=>local]] due to an existing alias with the same name
[2016-03-30 14:00:56,335][WARN ][org.elasticsearch.index.shard] [node_s1] [test][5] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 14:00:56,355][WARN ][org.elasticsearch.index.shard] [node_s1] [test][7] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 14:00:56,371][WARN ][org.elasticsearch.index.shard] [node_s1] [test][3] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 14:00:56,391][WARN ][org.elasticsearch.index.shard] [node_s1] [test][1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 14:00:56,426][WARN ][org.elasticsearch.index.shard] [node_s0] [test][4] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 14:00:56,446][WARN ][org.elasticsearch.index.shard] [node_s0] [test][6] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 14:00:56,456][WARN ][org.elasticsearch.index.shard] [node_s0] [test][2] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 14:00:56,472][WARN ][org.elasticsearch.index.shard] [node_s0] [test][0] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 14:00:56,549][WARN ][org.elasticsearch.index.shard] [node_s0] [test][8] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
[2016-03-30 14:00:57,343][ERROR][org.elasticsearch.update ] Actual version [93] Expected version [93] Total failures [124]
[2016-03-30 14:00:57,344][ERROR][org.elasticsearch.update ] Actual version [13] Expected version [13] Total failures [204]
[2016-03-30 14:00:57,345][ERROR][org.elasticsearch.update ] Actual version [95] Expected version [95] Total failures [122]
[2016-03-30 14:00:57,346][ERROR][org.elasticsearch.update ] Actual version [46] Expected version [46] Total failures [171]
[2016-03-30 14:00:57,347][ERROR][org.elasticsearch.update ] Actual version [1] Expected version [1] Total failures [216]
[2016-03-30 14:00:57,348][ERROR][org.elasticsearch.update ] Actual version [34] Expected version [34] Total failures [183]
[2016-03-30 14:01:17,887][INFO ][org.elasticsearch.indices.recovery] [IndexRecoveryIT#testRerouteRecovery]: setup test
[2016-03-30 14:01:17,890][INFO ][org.elasticsearch.indices.recovery] [IndexRecoveryIT#testRerouteRecovery]: starting test
[2016-03-30 14:01:17,891][INFO ][org.elasticsearch.indices.recovery] --> start node A
[2016-03-30 14:01:17,947][DEBUG][org.elasticsearch.indices.recovery] [node_t0] using max_bytes_per_sec[28mb], concurrent_streams [5], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:01:17,960][INFO ][org.elasticsearch.indices.recovery] --> create index on node: node_t0
[2016-03-30 14:01:17,960][INFO ][org.elasticsearch.indices.recovery] --> creating test index: test-idx-1
[2016-03-30 14:01:18,039][DEBUG][org.elasticsearch.indices.recovery] indices [_all] are green
[2016-03-30 14:01:18,039][INFO ][org.elasticsearch.indices.recovery] --> indexing sample data
[2016-03-30 14:01:18,078][INFO ][org.elasticsearch.indices.recovery] Index [777] docs async: [false] bulk: [true] partitions [6]
[2016-03-30 14:01:19,219][INFO ][org.elasticsearch.indices.recovery] --> start node B
[2016-03-30 14:01:19,279][DEBUG][org.elasticsearch.indices.recovery] [node_t1] using max_bytes_per_sec[40mb], concurrent_streams [5], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:01:19,323][DEBUG][org.elasticsearch.indices.recovery] indices [_all] are green
[2016-03-30 14:01:19,323][INFO ][org.elasticsearch.indices.recovery] --> slowing down recoveries
[2016-03-30 14:01:19,325][INFO ][org.elasticsearch.indices.recovery] [node_t1] updating [indices.recovery.max_bytes_per_sec] from [40mb] to [17.6kb]
[2016-03-30 14:01:19,334][INFO ][org.elasticsearch.indices.recovery] [node_t1] updating [indices.recovery.file_chunk_size] from [512kb] to [17.6kb]
[2016-03-30 14:01:19,335][INFO ][org.elasticsearch.indices.recovery] [node_t0] updating [indices.recovery.max_bytes_per_sec] from [28mb] to [17.6kb]
[2016-03-30 14:01:19,335][INFO ][org.elasticsearch.indices.recovery] [node_t0] updating [indices.recovery.file_chunk_size] from [512kb] to [17.6kb]
[2016-03-30 14:01:19,337][INFO ][org.elasticsearch.indices.recovery] --> move shard from: node_t0 to: node_t1
[2016-03-30 14:01:19,364][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] started recovery from {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local], id [210]
[2016-03-30 14:01:19,364][TRACE][org.elasticsearch.indices.recovery] [node_t1] collecting local files for [test-idx-1][0] [210]
[2016-03-30 14:01:19,384][INFO ][org.elasticsearch.indices.recovery] --> waiting for recovery to start both on source and target
[2016-03-30 14:01:19,386][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] starting recovery from {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]
[2016-03-30 14:01:19,386][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] starting recovery to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local], mark_as_relocated false
[2016-03-30 14:01:19,386][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [2] for recovery
[2016-03-30 14:01:19,390][INFO ][org.elasticsearch.indices.recovery] --> request recoveries
[2016-03-30 14:01:19,391][INFO ][org.elasticsearch.indices.recovery] --> request node recovery stats
[2016-03-30 14:01:19,400][INFO ][org.elasticsearch.indices.recovery] --> checking throttling increases
[2016-03-30 14:01:19,421][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase1] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: recovering [_0.cfe], does not exists in remote
[2016-03-30 14:01:19,421][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase1] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: recovering [_0.si], does not exists in remote
[2016-03-30 14:01:19,421][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase1] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: recovering [_0.cfs], does not exists in remote
[2016-03-30 14:01:19,421][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase1] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: recovering [segments_2], does not exists in remote
[2016-03-30 14:01:19,421][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase1] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: recovering_files [4] with total_size [176.6kb], reusing_files [0] with total_size [0b]
[2016-03-30 14:01:19,979][INFO ][org.elasticsearch.indices.recovery] --> speeding up recoveries
[2016-03-30 14:01:19,980][INFO ][org.elasticsearch.indices.recovery] [node_t1] updating [indices.recovery.max_bytes_per_sec] from [17.6kb] to [20mb]
[2016-03-30 14:01:19,981][INFO ][org.elasticsearch.indices.recovery] [node_t1] updating [indices.recovery.file_chunk_size] from [17.6kb] to [512kb]
[2016-03-30 14:01:19,982][INFO ][org.elasticsearch.indices.recovery] [node_t0] updating [indices.recovery.max_bytes_per_sec] from [17.6kb] to [20mb]
[2016-03-30 14:01:19,982][INFO ][org.elasticsearch.indices.recovery] [node_t0] updating [indices.recovery.file_chunk_size] from [17.6kb] to [512kb]
[2016-03-30 14:01:20,738][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase1] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: prepare remote engine for translog
[2016-03-30 14:01:20,820][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase1] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: remote engine start took [81.4ms]
[2016-03-30 14:01:20,820][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase1] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: took [0s]
[2016-03-30 14:01:20,820][TRACE][org.elasticsearch.indices.recovery] [node_t0] snapshot translog for recovery. current size is [0]
[2016-03-30 14:01:20,820][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase2] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: sending transaction log operations
[2016-03-30 14:01:20,820][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] no translog operations to send to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]
[2016-03-30 14:01:20,820][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] sent final batch of [0][0b] (total: [0]) translog operations to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]
[2016-03-30 14:01:20,820][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery [phase2] to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: took [309.6micros]
[2016-03-30 14:01:20,820][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] finalizing recovery to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]
[2016-03-30 14:01:20,821][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] finalizing recovery to {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]: took [587.2micros]
[2016-03-30 14:01:20,821][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] marking recovery from {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local] as done, id [210]
[2016-03-30 14:01:20,823][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery completed from {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local], took[1.4s]
   phase1: recovered_files [4] with total_size of [176.6kb], took [0s], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [81ms]
         : recovered [0] transaction log operations, took [0s]

[2016-03-30 14:01:20,936][DEBUG][org.elasticsearch.indices.recovery] indices [_all] are green
[2016-03-30 14:01:20,937][INFO ][org.elasticsearch.indices.recovery] --> bump replica count
[2016-03-30 14:01:20,987][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] started recovery from {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local], id [211]
[2016-03-30 14:01:20,987][TRACE][org.elasticsearch.indices.recovery] [node_t0] collecting local files for [test-idx-1][0] [211]
[2016-03-30 14:01:20,991][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] starting recovery from {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]
[2016-03-30 14:01:20,991][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] starting recovery to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local], mark_as_relocated false
[2016-03-30 14:01:20,991][TRACE][org.elasticsearch.indices.recovery] [node_t1] captured translog id [1] for recovery
[2016-03-30 14:01:21,008][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: recovering [_0.cfe], does not exists in remote
[2016-03-30 14:01:21,008][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: recovering [_0.si], does not exists in remote
[2016-03-30 14:01:21,008][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: recovering [_0.cfs], does not exists in remote
[2016-03-30 14:01:21,008][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: recovering [segments_3], does not exists in remote
[2016-03-30 14:01:21,008][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: recovering_files [4] with total_size [176.6kb], reusing_files [0] with total_size [0b]
[2016-03-30 14:01:21,067][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: prepare remote engine for translog
[2016-03-30 14:01:21,090][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: remote engine start took [23.2ms]
[2016-03-30 14:01:21,090][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: took [0s]
[2016-03-30 14:01:21,090][TRACE][org.elasticsearch.indices.recovery] [node_t1] snapshot translog for recovery. current size is [0]
[2016-03-30 14:01:21,090][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase2] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: sending transaction log operations
[2016-03-30 14:01:21,090][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] no translog operations to send to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]
[2016-03-30 14:01:21,091][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] sent final batch of [0][0b] (total: [0]) translog operations to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]
[2016-03-30 14:01:21,091][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase2] to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: took [243micros]
[2016-03-30 14:01:21,091][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] finalizing recovery to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]
[2016-03-30 14:01:21,091][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] finalizing recovery to {node_t0}{VGm5gavKS7afi73yz9Cc4Q}{local}{local[276]}[mode=>local]: took [560.7micros]
[2016-03-30 14:01:21,092][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] marking recovery from {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local] as done, id [211]
[2016-03-30 14:01:21,092][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test-idx-1][0] recovery completed from {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local], took[104ms]
   phase1: recovered_files [4] with total_size of [176.6kb], took [0s], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [23ms]
         : recovered [0] transaction log operations, took [0s]

[2016-03-30 14:01:21,122][DEBUG][org.elasticsearch.indices.recovery] indices [_all] are green
[2016-03-30 14:01:21,123][INFO ][org.elasticsearch.indices.recovery] --> start node C
[2016-03-30 14:01:21,185][DEBUG][org.elasticsearch.indices.recovery] [node_t2] using max_bytes_per_sec[40mb], concurrent_streams [5], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [false]
[2016-03-30 14:01:21,202][INFO ][org.elasticsearch.indices.recovery] [node_t2] updating [indices.recovery.max_bytes_per_sec] from [40mb] to [20mb]
[2016-03-30 14:01:21,231][INFO ][org.elasticsearch.indices.recovery] --> slowing down recoveries
[2016-03-30 14:01:21,232][INFO ][org.elasticsearch.indices.recovery] [node_t1] updating [indices.recovery.max_bytes_per_sec] from [20mb] to [17.6kb]
[2016-03-30 14:01:21,232][INFO ][org.elasticsearch.indices.recovery] [node_t1] updating [indices.recovery.file_chunk_size] from [512kb] to [17.6kb]
[2016-03-30 14:01:21,232][INFO ][org.elasticsearch.indices.recovery] [node_t2] updating [indices.recovery.max_bytes_per_sec] from [20mb] to [17.6kb]
[2016-03-30 14:01:21,232][INFO ][org.elasticsearch.indices.recovery] [node_t2] updating [indices.recovery.file_chunk_size] from [512kb] to [17.6kb]
[2016-03-30 14:01:21,233][INFO ][org.elasticsearch.indices.recovery] [node_t0] updating [indices.recovery.max_bytes_per_sec] from [20mb] to [17.6kb]
[2016-03-30 14:01:21,233][INFO ][org.elasticsearch.indices.recovery] [node_t0] updating [indices.recovery.file_chunk_size] from [512kb] to [17.6kb]
[2016-03-30 14:01:21,241][INFO ][org.elasticsearch.indices.recovery] --> move replica shard from: node_t0 to: node_t2
[2016-03-30 14:01:21,259][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test-idx-1][0] started recovery from {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local], id [212]
[2016-03-30 14:01:21,259][TRACE][org.elasticsearch.indices.recovery] [node_t2] collecting local files for [test-idx-1][0] [212]
[2016-03-30 14:01:21,265][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test-idx-1][0] starting recovery from {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local]
[2016-03-30 14:01:21,266][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] starting recovery to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local], mark_as_relocated false
[2016-03-30 14:01:21,266][TRACE][org.elasticsearch.indices.recovery] [node_t1] captured translog id [1] for recovery
[2016-03-30 14:01:21,266][INFO ][org.elasticsearch.indices.recovery] --> speeding up recoveries
[2016-03-30 14:01:21,277][INFO ][org.elasticsearch.indices.recovery] [node_t1] updating [indices.recovery.max_bytes_per_sec] from [17.6kb] to [20mb]
[2016-03-30 14:01:21,277][INFO ][org.elasticsearch.indices.recovery] [node_t1] updating [indices.recovery.file_chunk_size] from [17.6kb] to [512kb]
[2016-03-30 14:01:21,280][INFO ][org.elasticsearch.indices.recovery] [node_t2] updating [indices.recovery.max_bytes_per_sec] from [17.6kb] to [20mb]
[2016-03-30 14:01:21,280][INFO ][org.elasticsearch.indices.recovery] [node_t2] updating [indices.recovery.file_chunk_size] from [17.6kb] to [512kb]
[2016-03-30 14:01:21,281][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: recovering [_0.cfe], does not exists in remote
[2016-03-30 14:01:21,281][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: recovering [_0.si], does not exists in remote
[2016-03-30 14:01:21,281][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: recovering [_0.cfs], does not exists in remote
[2016-03-30 14:01:21,281][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: recovering [segments_3], does not exists in remote
[2016-03-30 14:01:21,281][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: recovering_files [4] with total_size [176.6kb], reusing_files [0] with total_size [0b]
[2016-03-30 14:01:21,281][INFO ][org.elasticsearch.indices.recovery] [node_t0] updating [indices.recovery.max_bytes_per_sec] from [17.6kb] to [20mb]
[2016-03-30 14:01:21,282][INFO ][org.elasticsearch.indices.recovery] [node_t0] updating [indices.recovery.file_chunk_size] from [17.6kb] to [512kb]
[2016-03-30 14:01:21,430][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: prepare remote engine for translog
[2016-03-30 14:01:21,465][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: remote engine start took [35.5ms]
[2016-03-30 14:01:21,465][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase1] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: took [0s]
[2016-03-30 14:01:21,465][TRACE][org.elasticsearch.indices.recovery] [node_t1] snapshot translog for recovery. current size is [0]
[2016-03-30 14:01:21,465][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase2] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: sending transaction log operations
[2016-03-30 14:01:21,466][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] no translog operations to send to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]
[2016-03-30 14:01:21,466][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] sent final batch of [0][0b] (total: [0]) translog operations to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]
[2016-03-30 14:01:21,466][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] recovery [phase2] to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: took [251.9micros]
[2016-03-30 14:01:21,466][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] finalizing recovery to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]
[2016-03-30 14:01:21,466][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test-idx-1][0] finalizing recovery to {node_t2}{pqc8RwI0TlW2lc0FiG_IVA}{local}{local[280]}[mode=>local]: took [542.4micros]
[2016-03-30 14:01:21,467][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test-idx-1][0] marking recovery from {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local] as done, id [212]
[2016-03-30 14:01:21,467][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test-idx-1][0] recovery completed from {node_t1}{DuYspanuS8CYK3hlQN2Njw}{local}{local[278]}[mode=>local], took[208ms]
   phase1: recovered_files [4] with total_size of [176.6kb], took [0s], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [35ms]
         : recovered [0] transaction log operations, took [0s]

[2016-03-30 14:01:21,579][DEBUG][org.elasticsearch.indices.recovery] indices [_all] are green
[2016-03-30 14:01:21,580][INFO ][org.elasticsearch.indices.recovery] [IndexRecoveryIT#testRerouteRecovery]: finished test
[2016-03-30 14:01:21,580][INFO ][org.elasticsearch.indices.recovery] [IndexRecoveryIT#testRerouteRecovery]: cleaning up after test
[2016-03-30 14:01:21,580][TRACE][org.elasticsearch.indices.recovery] Check consistency for [3] nodes
[2016-03-30 14:01:21,695][INFO ][org.elasticsearch.indices.recovery] [IndexRecoveryIT#testRerouteRecovery]: cleaned up after test
[2016-03-30 14:01:47,144][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 14:01:51,894][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 14:01:55,906][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 14:01:57,950][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 14:02:01,152][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 14:02:02,849][WARN ][org.elasticsearch.discovery] [tribe_node] waited for 0s and no initial state was set by the discovery
[2016-03-30 14:02:56,730][ERROR][org.elasticsearch.gateway] [node_t0] failed to read local state, exiting...
ElasticsearchException[java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?]; nested: IllegalStateException[unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?];
	at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:163)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:309)
	at org.elasticsearch.gateway.MetaStateService.loadIndexState(MetaStateService.java:112)
	at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:97)
	at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:101)
	at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:227)
	at org.elasticsearch.gateway.GatewayMetaState.<init>(GatewayMetaState.java:89)
	at sun.reflect.GeneratedConstructorAccessor424.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
	at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
	at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:205)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:187)
	at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
	at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
	at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
	at org.elasticsearch.node.Node.<init>(Node.java:203)
	at org.elasticsearch.node.MockNode.<init>(MockNode.java:43)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:601)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:580)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1456)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1449)
	at org.elasticsearch.bwcompat.RecoveryWithUnsupportedIndicesIT.testUpgradeStartClusterOn_0_20_6(RecoveryWithUnsupportedIndicesIT.java:33)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?
	at org.elasticsearch.common.compress.CompressorFactory.compressor(CompressorFactory.java:84)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:101)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:123)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.fromXContent(IndexMetaData.java:916)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:186)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:294)
	... 88 more
[2016-03-30 14:02:56,744][ERROR][org.elasticsearch.gateway] [node_t0] failed to read local state, exiting...
ElasticsearchException[java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?]; nested: IllegalStateException[unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?];
	at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:163)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:309)
	at org.elasticsearch.gateway.MetaStateService.loadIndexState(MetaStateService.java:112)
	at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:97)
	at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:101)
	at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:227)
	at org.elasticsearch.gateway.GatewayMetaState.<init>(GatewayMetaState.java:89)
	at sun.reflect.GeneratedConstructorAccessor424.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
	at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:205)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:187)
	at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
	at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
	at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
	at org.elasticsearch.node.Node.<init>(Node.java:203)
	at org.elasticsearch.node.MockNode.<init>(MockNode.java:43)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:601)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:580)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1456)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1449)
	at org.elasticsearch.bwcompat.RecoveryWithUnsupportedIndicesIT.testUpgradeStartClusterOn_0_20_6(RecoveryWithUnsupportedIndicesIT.java:33)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?
	at org.elasticsearch.common.compress.CompressorFactory.compressor(CompressorFactory.java:84)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:101)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:123)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.fromXContent(IndexMetaData.java:916)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:186)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:294)
	... 79 more
[2016-03-30 14:02:56,760][ERROR][org.elasticsearch.gateway] [node_t0] failed to read local state, exiting...
ElasticsearchException[java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?]; nested: IllegalStateException[unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?];
	at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:163)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:309)
	at org.elasticsearch.gateway.MetaStateService.loadIndexState(MetaStateService.java:112)
	at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:97)
	at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:101)
	at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:227)
	at org.elasticsearch.gateway.GatewayMetaState.<init>(GatewayMetaState.java:89)
	at sun.reflect.GeneratedConstructorAccessor424.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:205)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:187)
	at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
	at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
	at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
	at org.elasticsearch.node.Node.<init>(Node.java:203)
	at org.elasticsearch.node.MockNode.<init>(MockNode.java:43)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:601)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:580)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1456)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1449)
	at org.elasticsearch.bwcompat.RecoveryWithUnsupportedIndicesIT.testUpgradeStartClusterOn_0_20_6(RecoveryWithUnsupportedIndicesIT.java:33)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?
	at org.elasticsearch.common.compress.CompressorFactory.compressor(CompressorFactory.java:84)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:101)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:123)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.fromXContent(IndexMetaData.java:916)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:186)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:294)
	... 70 more
[2016-03-30 14:02:56,772][ERROR][org.elasticsearch.gateway] [node_t0] failed to read local state, exiting...
ElasticsearchException[java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?]; nested: IllegalStateException[unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?];
	at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:163)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:309)
	at org.elasticsearch.gateway.MetaStateService.loadIndexState(MetaStateService.java:112)
	at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:97)
	at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:101)
	at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:227)
	at org.elasticsearch.gateway.GatewayMetaState.<init>(GatewayMetaState.java:89)
	at sun.reflect.GeneratedConstructorAccessor424.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
	at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:205)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:190)
	at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
	at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
	at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
	at org.elasticsearch.node.Node.<init>(Node.java:203)
	at org.elasticsearch.node.MockNode.<init>(MockNode.java:43)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:601)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:580)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1456)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1449)
	at org.elasticsearch.bwcompat.RecoveryWithUnsupportedIndicesIT.testUpgradeStartClusterOn_0_20_6(RecoveryWithUnsupportedIndicesIT.java:33)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?
	at org.elasticsearch.common.compress.CompressorFactory.compressor(CompressorFactory.java:84)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:101)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:123)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.fromXContent(IndexMetaData.java:916)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:186)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:294)
	... 79 more
[2016-03-30 14:02:56,786][ERROR][org.elasticsearch.gateway] [node_t0] failed to read local state, exiting...
ElasticsearchException[java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?]; nested: IllegalStateException[unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?];
	at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:163)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:309)
	at org.elasticsearch.gateway.MetaStateService.loadIndexState(MetaStateService.java:112)
	at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:97)
	at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:101)
	at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:227)
	at org.elasticsearch.gateway.GatewayMetaState.<init>(GatewayMetaState.java:89)
	at sun.reflect.GeneratedConstructorAccessor424.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:205)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:190)
	at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
	at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
	at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
	at org.elasticsearch.node.Node.<init>(Node.java:203)
	at org.elasticsearch.node.MockNode.<init>(MockNode.java:43)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:601)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:580)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1456)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1449)
	at org.elasticsearch.bwcompat.RecoveryWithUnsupportedIndicesIT.testUpgradeStartClusterOn_0_20_6(RecoveryWithUnsupportedIndicesIT.java:33)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?
	at org.elasticsearch.common.compress.CompressorFactory.compressor(CompressorFactory.java:84)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:101)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:123)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.fromXContent(IndexMetaData.java:916)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:186)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:294)
	... 70 more
[2016-03-30 14:02:56,795][ERROR][org.elasticsearch.gateway] [node_t0] failed to read local state, exiting...
ElasticsearchException[java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?]; nested: IllegalStateException[unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?];
	at org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress(ExceptionsHelper.java:163)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:309)
	at org.elasticsearch.gateway.MetaStateService.loadIndexState(MetaStateService.java:112)
	at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:97)
	at org.elasticsearch.gateway.GatewayMetaState.loadMetaState(GatewayMetaState.java:101)
	at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:227)
	at org.elasticsearch.gateway.GatewayMetaState.<init>(GatewayMetaState.java:89)
	at sun.reflect.GeneratedConstructorAccessor424.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
	at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
	at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
	at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
	at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
	at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
	at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
	at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:205)
	at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:197)
	at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:190)
	at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
	at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96)
	at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
	at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
	at org.elasticsearch.node.Node.<init>(Node.java:203)
	at org.elasticsearch.node.MockNode.<init>(MockNode.java:43)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:601)
	at org.elasticsearch.test.InternalTestCluster.buildNode(InternalTestCluster.java:580)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1456)
	at org.elasticsearch.test.InternalTestCluster.startNode(InternalTestCluster.java:1449)
	at org.elasticsearch.bwcompat.RecoveryWithUnsupportedIndicesIT.testUpgradeStartClusterOn_0_20_6(RecoveryWithUnsupportedIndicesIT.java:33)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1660)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:866)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:902)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:916)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:875)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:777)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:811)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:822)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: unsupported compression: index was created before v2.0.0.beta1 and wasn't upgraded?
	at org.elasticsearch.common.compress.CompressorFactory.compressor(CompressorFactory.java:84)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:101)
	at org.elasticsearch.common.compress.CompressedXContent.<init>(CompressedXContent.java:123)
	at org.elasticsearch.cluster.metadata.IndexMetaData$Builder.fromXContent(IndexMetaData.java:916)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:186)
	at org.elasticsearch.gateway.MetaStateService$2.fromXContent(MetaStateService.java:178)
	at org.elasticsearch.gateway.MetaDataStateFormat.loadLatestState(MetaDataStateFormat.java:294)
	... 88 more
[2016-03-30 14:03:00,112][WARN ][org.elasticsearch.discovery.zen] [node_t1] master left (reason = shut_down), current nodes: {{node_t1}{f2h0o8M-QXWcbMsNmcyZfQ}{127.0.0.1}{127.0.0.1:9441}[mode=>network],}
[2016-03-30 14:03:01,013][WARN ][org.elasticsearch.discovery.zen] [node_t1] not enough master nodes, current nodes: {{node_t1}{f2h0o8M-QXWcbMsNmcyZfQ}{127.0.0.1}{127.0.0.1:9441}[mode=>network],}
[2016-03-30 14:03:01,016][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9440}]
RemoteTransportException[[node_t2][127.0.0.1:9440][internal:discovery/zen/unicast]]; nested: IllegalStateException[received ping request while not started];
Caused by: java.lang.IllegalStateException: received ping request while not started
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:497)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:83)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:522)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:518)
	at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:244)
	at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:03:01,017][WARN ][org.elasticsearch.discovery.zen.ping.unicast] [node_t1] failed to send ping to [{#zen_unicast_6#}{::1}{[::1]:9440}]
RemoteTransportException[[node_t2][[::1]:9440][internal:discovery/zen/unicast]]; nested: IllegalStateException[received ping request while not started];
Caused by: java.lang.IllegalStateException: received ping request while not started
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:497)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:83)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:522)
	at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:518)
	at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:244)
	at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:03:01,514][WARN ][org.elasticsearch.discovery.zen] [node_t3] master left (reason = shut_down), current nodes: {{node_t3}{qoDH7YEhRz6Y4lgMS7dPZg}{127.0.0.1}{127.0.0.1:9440}[mode=>network],}
[2016-03-30 14:03:02,248][WARN ][org.elasticsearch.cluster.service] [node_t0] cluster state update task [test2] took 104ms above the warn threshold of 10ms
[2016-03-30 14:03:02,249][WARN ][org.elasticsearch.cluster.service] [node_t0] cluster state update task [test2] took 105ms above the warn threshold of 10ms
[2016-03-30 14:03:02,350][WARN ][org.elasticsearch.cluster.service] [node_t0] cluster state update task [test3] took 100ms above the warn threshold of 10ms
[2016-03-30 14:03:02,451][WARN ][org.elasticsearch.cluster.service] [node_t0] cluster state update task [test4] took 100ms above the warn threshold of 10ms
[2016-03-30 14:03:02,787][INFO ][org.elasticsearch.cluster] [ClusterServiceIT#testClusterStateUpdateLogging]: setup test
[2016-03-30 14:03:02,790][INFO ][org.elasticsearch.cluster] [ClusterServiceIT#testClusterStateUpdateLogging]: starting test
[2016-03-30 14:03:02,890][DEBUG][org.elasticsearch.cluster.routing.allocation.decider] [node_t0] using node_concurrent_recoveries [5], node_initial_primaries_recoveries [4]
[2016-03-30 14:03:02,890][DEBUG][org.elasticsearch.cluster.routing.allocation.decider] [node_t0] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2016-03-30 14:03:02,891][DEBUG][org.elasticsearch.cluster.routing.allocation.decider] [node_t0] using [cluster_concurrent_rebalance] with [2]
[2016-03-30 14:03:02,940][INFO ][org.elasticsearch.cluster] [node_t0] initialized test service
[2016-03-30 14:03:02,950][TRACE][org.elasticsearch.cluster.service] [node_t0] will process [local-disco-initial_connect(master)]
[2016-03-30 14:03:02,950][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [local-disco-initial_connect(master)]: execute
[2016-03-30 14:03:02,951][TRACE][org.elasticsearch.cluster.service] [node_t0] cluster state updated, source [local-disco-initial_connect(master)]
version: 1
state uuid: NxMyVNXUSbSTNc8H1f2dHw
from_diff: false
meta data version: 0
blocks: 
   _global_:
      1,state not recovered / initialized, blocks READ,WRITE,METADATA_READ,METADATA_WRITE
nodes: 
   {node_t0}{jPnknTyiTyyBYSxlepW7iw}{127.0.0.1}{127.0.0.1:9460}[mode=>network], local, master
routing_table (version 0):
routing_nodes:
-----node_id[jPnknTyiTyyBYSxlepW7iw][V]
---- unassigned

[2016-03-30 14:03:02,951][INFO ][org.elasticsearch.cluster.service] [node_t0] new_master {node_t0}{jPnknTyiTyyBYSxlepW7iw}{127.0.0.1}{127.0.0.1:9460}[mode=>network], reason: local-disco-initial_connect(master)
[2016-03-30 14:03:02,951][DEBUG][org.elasticsearch.cluster.service] [node_t0] publishing cluster state version [1]
[2016-03-30 14:03:02,951][DEBUG][org.elasticsearch.cluster.service] [node_t0] set local cluster state to version 1
[2016-03-30 14:03:02,951][TRACE][org.elasticsearch.cluster.routing] [node_t0] no need to schedule reroute - no delayed unassigned shards, minDelaySetting [0], scheduled [9223372036854775807]
[2016-03-30 14:03:02,952][INFO ][org.elasticsearch.cluster] [node_t0] on master [{node_t0}{jPnknTyiTyyBYSxlepW7iw}{127.0.0.1}{127.0.0.1:9460}[mode=>network]]
[2016-03-30 14:03:02,952][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [local-disco-initial_connect(master)]: took 1ms done applying updated cluster_state (version: 1, uuid: NxMyVNXUSbSTNc8H1f2dHw)
[2016-03-30 14:03:02,952][TRACE][org.elasticsearch.cluster] [node_t0] I have been elected master, scheduling a ClusterInfoUpdateJob
[2016-03-30 14:03:02,952][TRACE][org.elasticsearch.cluster.service] [node_t0] will process [gateway_initial_state_recovery]
[2016-03-30 14:03:02,953][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [gateway_initial_state_recovery]: execute
[2016-03-30 14:03:02,953][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [gateway_initial_state_recovery]: took 0s no change in cluster_state
[2016-03-30 14:03:02,954][TRACE][org.elasticsearch.cluster.service] [node_t0] will process [test1]
[2016-03-30 14:03:02,954][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [test1]: execute
[2016-03-30 14:03:02,954][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [test1]: took 0s no change in cluster_state
[2016-03-30 14:03:02,954][TRACE][org.elasticsearch.cluster.service] [node_t0] will process [test2]
[2016-03-30 14:03:02,954][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [test2]: execute
[2016-03-30 14:03:02,954][TRACE][org.elasticsearch.cluster.service] [node_t0] failed to execute cluster state update in 0s, state:
version [1], source [test2]
nodes: 
   {node_t0}{jPnknTyiTyyBYSxlepW7iw}{127.0.0.1}{127.0.0.1:9460}[mode=>network], local, master
routing_table (version 0):
routing_nodes:
-----node_id[jPnknTyiTyyBYSxlepW7iw][V]
---- unassigned

java.lang.IllegalArgumentException: Testing handling of exceptions in the cluster state task
	at org.elasticsearch.cluster.ClusterServiceIT$16.execute(ClusterServiceIT.java:885)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:03:02,955][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [test2]: took 0s no change in cluster_state
[2016-03-30 14:03:02,955][TRACE][org.elasticsearch.cluster.service] [node_t0] will process [test3]
[2016-03-30 14:03:02,955][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [test3]: execute
[2016-03-30 14:03:02,955][TRACE][org.elasticsearch.cluster.service] [node_t0] cluster state updated, source [test3]
version: 3
state uuid: dJ8qmEVhRGibCvh4w_v6sg
from_diff: false
meta data version: 0
blocks: 
   _global_:
      1,state not recovered / initialized, blocks READ,WRITE,METADATA_READ,METADATA_WRITE
nodes: 
   {node_t0}{jPnknTyiTyyBYSxlepW7iw}{127.0.0.1}{127.0.0.1:9460}[mode=>network], local, master
routing_table (version 0):
routing_nodes:
-----node_id[jPnknTyiTyyBYSxlepW7iw][V]
---- unassigned

[2016-03-30 14:03:02,955][DEBUG][org.elasticsearch.cluster.service] [node_t0] publishing cluster state version [3]
[2016-03-30 14:03:02,955][DEBUG][org.elasticsearch.cluster.service] [node_t0] set local cluster state to version 3
[2016-03-30 14:03:02,955][TRACE][org.elasticsearch.cluster.routing] [node_t0] no need to schedule reroute - no delayed unassigned shards, minDelaySetting [0], scheduled [9223372036854775807]
[2016-03-30 14:03:02,955][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [test3]: took 0s done applying updated cluster_state (version: 3, uuid: dJ8qmEVhRGibCvh4w_v6sg)
[2016-03-30 14:03:02,955][TRACE][org.elasticsearch.cluster.service] [node_t0] will process [test4]
[2016-03-30 14:03:02,955][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [test4]: execute
[2016-03-30 14:03:02,956][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [test4]: took 0s no change in cluster_state
[2016-03-30 14:03:02,956][TRACE][org.elasticsearch.cluster.service] [node_t0] will process [local-gateway-elected-state]
[2016-03-30 14:03:02,956][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [local-gateway-elected-state]: execute
[2016-03-30 14:03:02,956][INFO ][org.elasticsearch.cluster] [ClusterServiceIT#testClusterStateUpdateLogging]: finished test
[2016-03-30 14:03:02,956][INFO ][org.elasticsearch.cluster] [ClusterServiceIT#testClusterStateUpdateLogging]: cleaning up after test
[2016-03-30 14:03:02,956][TRACE][org.elasticsearch.cluster.routing.allocation.allocator] [node_t0] Start assigning unassigned shards
[2016-03-30 14:03:02,957][TRACE][org.elasticsearch.cluster] Check consistency for [1] nodes
[2016-03-30 14:03:02,957][TRACE][org.elasticsearch.cluster.routing.allocation.allocator] [node_t0] Start distributing Shards
[2016-03-30 14:03:02,957][TRACE][org.elasticsearch.cluster.routing.allocation.allocator] [node_t0] Start allocating unassigned shards
[2016-03-30 14:03:02,957][TRACE][org.elasticsearch.cluster.service] [node_t0] cluster state updated, source [local-gateway-elected-state]
version: 4
state uuid: J_T6TOzcSRmDC0iO_zheew
from_diff: false
meta data version: 1
nodes: 
   {node_t0}{jPnknTyiTyyBYSxlepW7iw}{127.0.0.1}{127.0.0.1:9460}[mode=>network], local, master
routing_table (version 1):
routing_nodes:
-----node_id[jPnknTyiTyyBYSxlepW7iw][V]
---- unassigned

[2016-03-30 14:03:02,957][DEBUG][org.elasticsearch.cluster.service] [node_t0] publishing cluster state version [4]
[2016-03-30 14:03:02,957][DEBUG][org.elasticsearch.cluster.service] [node_t0] set local cluster state to version 4
[2016-03-30 14:03:02,957][TRACE][org.elasticsearch.cluster.routing] [node_t0] no need to schedule reroute - no delayed unassigned shards, minDelaySetting [0], scheduled [9223372036854775807]
[2016-03-30 14:03:02,975][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [local-gateway-elected-state]: took 18ms done applying updated cluster_state (version: 4, uuid: J_T6TOzcSRmDC0iO_zheew)
[2016-03-30 14:03:02,975][TRACE][org.elasticsearch.cluster.service] [node_t0] will process [delete_repository [*]]
[2016-03-30 14:03:02,975][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [delete_repository [*]]: execute
[2016-03-30 14:03:02,975][DEBUG][org.elasticsearch.cluster.service] [node_t0] processing [delete_repository [*]]: took 0s no change in cluster_state
[2016-03-30 14:03:02,980][INFO ][org.elasticsearch.cluster] [ClusterServiceIT#testClusterStateUpdateLogging]: cleaned up after test
[2016-03-30 14:03:05,118][WARN ][org.elasticsearch.cluster] [node_t0] even_shard allocator has been removed in 2.0 using balanced instead
[2016-03-30 14:03:05,164][WARN ][org.elasticsearch.test.transport] [node_t0] Transport response handler not found of id [0]
[2016-03-30 14:03:05,164][WARN ][org.elasticsearch.gateway] [node_t0] failed to fetch state from node
FailedNodeException[Failed node [NAA_Y9ETRXio3Z4bFnIi6g]]; nested: TransportException[transport stopped, action: internal:gateway/local/meta_state[n]];
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.onFailure(TransportNodesAction.java:188)
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$700(TransportNodesAction.java:95)
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction$2.handleException(TransportNodesAction.java:161)
	at org.elasticsearch.transport.TransportService$2.run(TransportService.java:198)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: TransportException[transport stopped, action: internal:gateway/local/meta_state[n]]
	... 4 more
[2016-03-30 14:03:47,352][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 19435 [18.9kb] for data of [test] would be larger than configured breaker: 100 [100b], breaking
[2016-03-30 14:03:48,466][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3412 [3.3kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,466][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3024 [2.9kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,465][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3273 [3.1kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,465][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3520 [3.4kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,467][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3711 [3.6kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,467][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3363 [3.2kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,468][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3567 [3.4kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,468][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3276 [3.1kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,468][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3320 [3.2kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,470][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3024 [2.9kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,470][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3567 [3.4kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,470][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3412 [3.3kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,470][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3711 [3.6kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,470][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3273 [3.1kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,470][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3363 [3.2kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,470][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3320 [3.2kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,470][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3276 [3.1kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:48,470][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 3520 [3.4kb] for data of [test] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:49,635][WARN ][org.elasticsearch.indices.breaker.fielddata] [fielddata] New used memory 26072 [25.4kb] for data of [test] would be larger than configured breaker: 100 [100b], breaking
[2016-03-30 14:03:50,691][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:50,691][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:50,691][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:50,691][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:50,692][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:50,691][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:50,692][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:50,692][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:50,692][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:50,693][WARN ][org.elasticsearch.indices.breaker.request] [request] New used memory 16440 [16kb] for data of [<reused_arrays>] would be larger than configured breaker: 10 [10b], breaking
[2016-03-30 14:03:51,490][WARN ][org.elasticsearch.indices.breaker.customBreaker] [customBreaker] New used memory 16 [16b] for data of [test] would be larger than configured breaker: 8 [8b], breaking
[2016-03-30 14:04:07,561][WARN ][org.elasticsearch.discovery.zen.publish] [node_t1] timed out waiting for all nodes to process published state [1] (timeout [0s], pending nodes: [{node_t0}{WDz2a7uGT_2vcu3b34tbLg}{127.0.0.1}{127.0.0.1:9400}[mode=>network]])
[2016-03-30 14:04:07,566][WARN ][org.elasticsearch.discovery.zen.publish] [node_t1] timed out waiting for all nodes to process published state [2] (timeout [0s], pending nodes: [{node_t0}{WDz2a7uGT_2vcu3b34tbLg}{127.0.0.1}{127.0.0.1:9400}[mode=>network]])
[2016-03-30 14:04:07,575][WARN ][org.elasticsearch.discovery.zen.publish] [node_t1] timed out waiting for all nodes to process published state [3] (timeout [0s], pending nodes: [{node_t0}{WDz2a7uGT_2vcu3b34tbLg}{127.0.0.1}{127.0.0.1:9400}[mode=>network]])
[2016-03-30 14:04:07,633][WARN ][org.elasticsearch.discovery.zen.publish] [node_t1] timed out waiting for all nodes to process published state [4] (timeout [0s], pending nodes: [{node_t0}{WDz2a7uGT_2vcu3b34tbLg}{127.0.0.1}{127.0.0.1:9400}[mode=>network]])
[2016-03-30 14:04:07,676][WARN ][org.elasticsearch.discovery.zen.publish] [node_t1] timed out waiting for all nodes to process published state [5] (timeout [0s], pending nodes: [{node_t0}{WDz2a7uGT_2vcu3b34tbLg}{127.0.0.1}{127.0.0.1:9400}[mode=>network]])
[2016-03-30 14:04:07,824][WARN ][org.elasticsearch.discovery.zen.publish] [node_t1] timed out waiting for all nodes to process published state [6] (timeout [0s], pending nodes: [{node_t0}{WDz2a7uGT_2vcu3b34tbLg}{127.0.0.1}{127.0.0.1:9400}[mode=>network]])
[2016-03-30 14:04:15,029][DEBUG][org.elasticsearch.gateway] [node_t0] using initial_shards [quorum]
[2016-03-30 14:04:15,054][DEBUG][org.elasticsearch.gateway] [node_t0] took 0s to load state
[2016-03-30 14:04:18,074][TRACE][org.elasticsearch.gateway] [node_t0] performing state recovery from [9B7XP-w4SDe-OX6gjQ3JSw]
[2016-03-30 14:04:18,075][TRACE][org.elasticsearch.gateway] [node_t0] successful state recovery, importing cluster state...
[2016-03-30 14:04:18,076][TRACE][org.elasticsearch.gateway] [node_t0] [_global] writing state, reason [changed]
[2016-03-30 14:04:18,083][INFO ][org.elasticsearch.gateway] [node_t0] recovered [0] indices into cluster_state
[2016-03-30 14:04:18,109][TRACE][org.elasticsearch.gateway] [node_t0] [index] writing state, reason [freshly created]
[2016-03-30 14:04:18,153][TRACE][org.elasticsearch.gateway] [node_t0] [index] writing state, reason [version changed from [1] to [2]]
[2016-03-30 14:04:18,171][TRACE][org.elasticsearch.gateway] [node_t0] [index] writing state, reason [version changed from [2] to [3]]
[2016-03-30 14:04:18,178][TRACE][org.elasticsearch.gateway] [node_t0] [index][0] fetching [shard_started] from [_non_existent, 9B7XP-w4SDe-OX6gjQ3JSw]
[2016-03-30 14:04:18,178][DEBUG][org.elasticsearch.gateway] [node_t0] failed to execute on node [_non_existent]
NoSuchNodeException[No such node [_non_existent]]
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.start(TransportNodesAction.java:140)
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$100(TransportNodesAction.java:95)
	at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:69)
	at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:45)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
	at org.elasticsearch.gateway.TransportNodesListGatewayStartedShards.list(TransportNodesListGatewayStartedShards.java:79)
	at org.elasticsearch.gateway.AsyncShardFetch.asyncFetch(AsyncShardFetch.java:275)
	at org.elasticsearch.gateway.AsyncShardFetch.fetchData(AsyncShardFetch.java:125)
	at org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator.fetchData(GatewayAllocator.java:157)
	at org.elasticsearch.gateway.PrimaryShardAllocator.allocateUnassigned(PrimaryShardAllocator.java:65)
	at org.elasticsearch.gateway.GatewayAllocator.allocateUnassigned(GatewayAllocator.java:121)
	at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:72)
	at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:304)
	at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:268)
	at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:254)
	at org.elasticsearch.indices.state.RareClusterStateIT$1.execute(RareClusterStateIT.java:131)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:04:18,179][TRACE][org.elasticsearch.gateway] [node_t0] [index][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-03-30T21:04:18.178Z]]: ignoring allocation, still fetching shard started state
[2016-03-30 14:04:18,179][TRACE][org.elasticsearch.gateway] [node_t0] [index][0] loading local shard state info
[2016-03-30 14:04:18,180][TRACE][org.elasticsearch.gateway] [node_t0] found state file: [id:0, legacy:false, file:/Users/brijeshs/Documents/elasticsearch_sort_netbeans/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.indices.state.RareClusterStateIT_F7928424F380CF7C-001/tempDir-004/data/TEST-CHILD_VM=[0]-CLUSTER_SEED=[2384909641678833841]-HASH=[462E1198545AF]-cluster/nodes/0/indices/index/0/_state/state-0.st]
[2016-03-30 14:04:18,180][WARN ][org.elasticsearch.cluster.service] [node_t0] failed to connect to node [{_non_existent}{dummy}{_dummy_addr_}]
ConnectTransportException[[][_dummy_addr_] general node connection failure]; nested: ClassCastException[org.elasticsearch.common.transport.DummyTransportAddress cannot be cast to org.elasticsearch.common.transport.InetSocketTransportAddress];
	at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:933)
	at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:889)
	at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.connectToNode(MockTransportService.java:407)
	at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:243)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:500)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: org.elasticsearch.common.transport.DummyTransportAddress cannot be cast to org.elasticsearch.common.transport.InetSocketTransportAddress
	at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:962)
	at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:916)
	... 10 more
[2016-03-30 14:04:18,182][TRACE][org.elasticsearch.gateway] [node_t0] state id [0] read from [state-0.st]
[2016-03-30 14:04:18,182][TRACE][org.elasticsearch.gateway] [node_t0] [index] writing state, reason [version changed from [3] to [4]]
[2016-03-30 14:04:18,182][TRACE][org.elasticsearch.gateway] [node_t0] found state file: [id:0, legacy:false, file:/Users/brijeshs/Documents/elasticsearch_sort_netbeans/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.indices.state.RareClusterStateIT_F7928424F380CF7C-001/tempDir-004/data/TEST-CHILD_VM=[0]-CLUSTER_SEED=[2384909641678833841]-HASH=[462E1198545AF]-cluster/nodes/0/indices/index/0/_state/state-0.st]
[2016-03-30 14:04:18,184][TRACE][org.elasticsearch.gateway] [node_t0] state id [0] read from [state-0.st]
[2016-03-30 14:04:18,184][DEBUG][org.elasticsearch.gateway] [node_t0] [index][0] loaded data path [/Users/brijeshs/Documents/elasticsearch_sort_netbeans/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.indices.state.RareClusterStateIT_F7928424F380CF7C-001/tempDir-004/data/TEST-CHILD_VM=[0]-CLUSTER_SEED=[2384909641678833841]-HASH=[462E1198545AF]-cluster/nodes/0/indices/index/0], state path [/Users/brijeshs/Documents/elasticsearch_sort_netbeans/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.indices.state.RareClusterStateIT_F7928424F380CF7C-001/tempDir-004/data/TEST-CHILD_VM=[0]-CLUSTER_SEED=[2384909641678833841]-HASH=[462E1198545AF]-cluster/nodes/0/indices/index/0]
[2016-03-30 14:04:18,187][DEBUG][org.elasticsearch.gateway] [node_t0] [index][0] shard state info found: [version [2], primary [true], allocation [[id=oe2Iynb0Suimxp-qbbr54A]]]
[2016-03-30 14:04:18,187][TRACE][org.elasticsearch.gateway] [node_t0] [index][0] processing fetched [shard_started] results
[2016-03-30 14:04:18,187][TRACE][org.elasticsearch.gateway] [node_t0] [index][0] marking 9B7XP-w4SDe-OX6gjQ3JSw as done for [shard_started]
[2016-03-30 14:04:18,187][TRACE][org.elasticsearch.gateway] [node_t0] [index][0] processing failure FailedNodeException[Failed node [_non_existent]]; nested: NoSuchNodeException[No such node [_non_existent]]; for [shard_started]
[2016-03-30 14:04:18,187][WARN ][org.elasticsearch.gateway] [node_t0] [index][0]: failed to list shard for shard_started on node [_non_existent]
FailedNodeException[Failed node [_non_existent]]; nested: NoSuchNodeException[No such node [_non_existent]];
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.onFailure(TransportNodesAction.java:188)
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.start(TransportNodesAction.java:140)
	at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$100(TransportNodesAction.java:95)
	at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:69)
	at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:45)
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
	at org.elasticsearch.gateway.TransportNodesListGatewayStartedShards.list(TransportNodesListGatewayStartedShards.java:79)
	at org.elasticsearch.gateway.AsyncShardFetch.asyncFetch(AsyncShardFetch.java:275)
	at org.elasticsearch.gateway.AsyncShardFetch.fetchData(AsyncShardFetch.java:125)
	at org.elasticsearch.gateway.GatewayAllocator$InternalPrimaryShardAllocator.fetchData(GatewayAllocator.java:157)
	at org.elasticsearch.gateway.PrimaryShardAllocator.allocateUnassigned(PrimaryShardAllocator.java:65)
	at org.elasticsearch.gateway.GatewayAllocator.allocateUnassigned(GatewayAllocator.java:121)
	at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:72)
	at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:304)
	at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:268)
	at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:254)
	at org.elasticsearch.indices.state.RareClusterStateIT$1.execute(RareClusterStateIT.java:131)
	at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
	at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:401)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: NoSuchNodeException[No such node [_non_existent]]
	... 24 more
[2016-03-30 14:04:18,188][TRACE][org.elasticsearch.gateway] [node_t0] [index][0] scheduling reroute for post_response
[2016-03-30 14:04:18,191][TRACE][org.elasticsearch.gateway] [node_t0] [[index][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-03-30T21:04:18.178Z]]] on node [{node_t0}{9B7XP-w4SDe-OX6gjQ3JSw}{127.0.0.1}{127.0.0.1:9460}[mode=>network]] has version [2] of shard
[2016-03-30 14:04:18,192][TRACE][org.elasticsearch.gateway] [node_t0] [index][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-03-30T21:04:18.178Z]] candidates for allocation: [[node_t0] -> 2, ]
[2016-03-30 14:04:18,192][DEBUG][org.elasticsearch.gateway] [node_t0] [index][0] found 1 allocations of [index][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-03-30T21:04:18.178Z]], highest version: [2]
[2016-03-30 14:04:18,193][DEBUG][org.elasticsearch.gateway] [node_t0] [index][0]: allocating [[index][0], node[null], [P], v[0], s[UNASSIGNED], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-03-30T21:04:18.178Z]]] to [{node_t0}{9B7XP-w4SDe-OX6gjQ3JSw}{127.0.0.1}{127.0.0.1:9460}[mode=>network]] on primary allocation
[2016-03-30 14:04:18,227][TRACE][org.elasticsearch.gateway] [node_t0] [index] writing state, reason [version changed from [4] to [5]]
[2016-03-30 14:05:08,640][DEBUG][org.elasticsearch.indices.recovery] [node_t0] using max_bytes_per_sec[40mb], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [false]
[2016-03-30 14:05:09,328][DEBUG][org.elasticsearch.indices.recovery] [node_t1] using max_bytes_per_sec[62mb], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:05:09,380][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] started recovery from {node_t0}{mvNcgmepQTebkA1i0WTA-g}{local}{local[466]}[mode=>local], id [443]
[2016-03-30 14:05:09,380][TRACE][org.elasticsearch.indices.recovery] [node_t1] collecting local files for [test][0] [443]
[2016-03-30 14:05:09,384][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] starting recovery from {node_t0}{mvNcgmepQTebkA1i0WTA-g}{local}{local[466]}[mode=>local]
[2016-03-30 14:05:09,384][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] starting recovery to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local], mark_as_relocated false
[2016-03-30 14:05:09,384][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [2] for recovery
[2016-03-30 14:05:09,391][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: recovering [_0.cfe], does not exists in remote
[2016-03-30 14:05:09,391][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: recovering [_0.si], does not exists in remote
[2016-03-30 14:05:09,391][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: recovering [_0.cfs], does not exists in remote
[2016-03-30 14:05:09,391][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: recovering [segments_2], does not exists in remote
[2016-03-30 14:05:09,391][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: recovering_files [4] with total_size [3.4kb], reusing_files [0] with total_size [0b]
[2016-03-30 14:05:09,492][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: prepare remote engine for translog
[2016-03-30 14:05:09,637][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: remote engine start took [145.7ms]
[2016-03-30 14:05:09,637][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: took [0s]
[2016-03-30 14:05:09,638][TRACE][org.elasticsearch.indices.recovery] [node_t0] snapshot translog for recovery. current size is [10]
[2016-03-30 14:05:09,638][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: sending transaction log operations
[2016-03-30 14:05:09,650][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] sent final batch of [10][440b] (total: [10]) translog operations to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]
[2016-03-30 14:05:09,650][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: took [12.4ms]
[2016-03-30 14:05:09,650][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]
[2016-03-30 14:05:09,859][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t1}{2Gscg4nlQTKisTfpKICeWA}{local}{local[468]}[mode=>local]: took [209.1ms]
[2016-03-30 14:05:09,860][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] marking recovery from {node_t0}{mvNcgmepQTebkA1i0WTA-g}{local}{local[466]}[mode=>local] as done, id [443]
[2016-03-30 14:05:09,860][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] recovery completed from {node_t0}{mvNcgmepQTebkA1i0WTA-g}{local}{local[466]}[mode=>local], took[480ms]
   phase1: recovered_files [4] with total_size of [3.4kb], took [0s], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [145ms]
         : recovered [10] transaction log operations, took [12ms]

[2016-03-30 14:05:10,114][DEBUG][org.elasticsearch.indices.recovery] [node_t0] using max_bytes_per_sec[113mb], concurrent_streams [4], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:05:10,220][DEBUG][org.elasticsearch.indices.recovery] [node_t1] using max_bytes_per_sec[55mb], concurrent_streams [4], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:05:10,271][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] started recovery from {node_t0}{oKHhyPeyTda9QQB8GOlFXQ}{local}{local[470]}[mode=>local], id [444]
[2016-03-30 14:05:10,272][TRACE][org.elasticsearch.indices.recovery] [node_t1] collecting local files for [test][0] [444]
[2016-03-30 14:05:10,273][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] starting recovery from {node_t0}{oKHhyPeyTda9QQB8GOlFXQ}{local}{local[470]}[mode=>local]
[2016-03-30 14:05:10,274][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] starting recovery to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local], mark_as_relocated false
[2016-03-30 14:05:10,274][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [1] for recovery
[2016-03-30 14:05:10,278][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]: recovering [segments_1], does not exists in remote
[2016-03-30 14:05:10,278][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]: recovering_files [1] with total_size [130b], reusing_files [0] with total_size [0b]
[2016-03-30 14:05:10,333][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]: prepare remote engine for translog
[2016-03-30 14:05:10,383][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]: remote engine start took [50.1ms]
[2016-03-30 14:05:10,383][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]: took [0s]
[2016-03-30 14:05:10,383][TRACE][org.elasticsearch.indices.recovery] [node_t0] snapshot translog for recovery. current size is [0]
[2016-03-30 14:05:10,383][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]: sending transaction log operations
[2016-03-30 14:05:10,383][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] no translog operations to send to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]
[2016-03-30 14:05:10,383][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] sent final batch of [0][0b] (total: [0]) translog operations to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]
[2016-03-30 14:05:10,384][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]: took [229.7micros]
[2016-03-30 14:05:10,384][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]
[2016-03-30 14:05:10,384][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t1}{CHV_XTyTTtuYOhKI0z0tiQ}{local}{local[472]}[mode=>local]: took [387.4micros]
[2016-03-30 14:05:10,384][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] marking recovery from {node_t0}{oKHhyPeyTda9QQB8GOlFXQ}{local}{local[470]}[mode=>local] as done, id [444]
[2016-03-30 14:05:10,384][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] recovery completed from {node_t0}{oKHhyPeyTda9QQB8GOlFXQ}{local}{local[470]}[mode=>local], took[112ms]
   phase1: recovered_files [1] with total_size of [130b], took [0s], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [50ms]
         : recovered [0] transaction log operations, took [0s]

[2016-03-30 14:05:10,472][DEBUG][org.elasticsearch.indices.recovery] [node_t2] using max_bytes_per_sec[40mb], concurrent_streams [4], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:05:14,596][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] started recovery from {node_t0}{oKHhyPeyTda9QQB8GOlFXQ}{local}{local[470]}[mode=>local], id [445]
[2016-03-30 14:05:14,596][TRACE][org.elasticsearch.indices.recovery] [node_t2] collecting local files for [test][0] [445]
[2016-03-30 14:05:14,602][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] starting recovery from {node_t0}{oKHhyPeyTda9QQB8GOlFXQ}{local}{local[470]}[mode=>local]
[2016-03-30 14:05:14,602][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] starting recovery to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local], mark_as_relocated false
[2016-03-30 14:05:14,602][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [1] for recovery
[2016-03-30 14:05:14,607][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]: recovering [segments_1], does not exists in remote
[2016-03-30 14:05:14,607][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]: recovering_files [1] with total_size [130b], reusing_files [0] with total_size [0b]
[2016-03-30 14:05:14,625][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]: prepare remote engine for translog
[2016-03-30 14:05:14,682][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]: remote engine start took [56.9ms]
[2016-03-30 14:05:14,682][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]: took [0s]
[2016-03-30 14:05:14,682][TRACE][org.elasticsearch.indices.recovery] [node_t0] snapshot translog for recovery. current size is [364]
[2016-03-30 14:05:14,682][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]: sending transaction log operations
[2016-03-30 14:05:14,888][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] sent final batch of [364][83.5kb] (total: [364]) translog operations to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]
[2016-03-30 14:05:14,888][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]: took [205.6ms]
[2016-03-30 14:05:14,888][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]
[2016-03-30 14:05:15,266][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t2}{9CeYdMzPT2yacT4OzdRsPw}{local}{local[473]}[mode=>local]: took [377.8ms]
[2016-03-30 14:05:15,266][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] marking recovery from {node_t0}{oKHhyPeyTda9QQB8GOlFXQ}{local}{local[470]}[mode=>local] as done, id [445]
[2016-03-30 14:05:15,266][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] recovery completed from {node_t0}{oKHhyPeyTda9QQB8GOlFXQ}{local}{local[470]}[mode=>local], took[670ms]
   phase1: recovered_files [1] with total_size of [130b], took [0s], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [56ms]
         : recovered [364] transaction log operations, took [205ms]

[2016-03-30 14:05:18,769][WARN ][org.elasticsearch.test.transport] [node_t1] Transport response handler not found of id [14]
[2016-03-30 14:05:18,863][DEBUG][org.elasticsearch.indices.recovery] [node_t0] using max_bytes_per_sec[40mb], concurrent_streams [4], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:05:19,031][DEBUG][org.elasticsearch.indices.recovery] [node_t1] using max_bytes_per_sec[15mb], concurrent_streams [4], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:05:19,159][DEBUG][org.elasticsearch.indices.recovery] [node_t2] using max_bytes_per_sec[40mb], concurrent_streams [4], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:05:20,221][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] started recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local], id [446]
[2016-03-30 14:05:20,221][TRACE][org.elasticsearch.indices.recovery] [node_t2] collecting local files for [test][0] [446]
[2016-03-30 14:05:20,222][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] starting recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local]
[2016-03-30 14:05:20,223][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] starting recovery to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local], mark_as_relocated false
[2016-03-30 14:05:20,223][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [2] for recovery
[2016-03-30 14:05:20,234][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_0.cfe], does not exists in remote
[2016-03-30 14:05:20,234][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_0.si], does not exists in remote
[2016-03-30 14:05:20,234][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_0.cfs], does not exists in remote
[2016-03-30 14:05:20,234][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_1.cfs], does not exists in remote
[2016-03-30 14:05:20,234][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_1.cfe], does not exists in remote
[2016-03-30 14:05:20,234][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_1.si], does not exists in remote
[2016-03-30 14:05:20,234][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [segments_2], does not exists in remote
[2016-03-30 14:05:20,234][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_0_1.liv], does not exists in remote
[2016-03-30 14:05:20,234][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering_files [8] with total_size [8.3kb], reusing_files [0] with total_size [0b]
[2016-03-30 14:05:20,427][DEBUG][org.elasticsearch.indices.recovery] [node_t0] Failed to transfer file [name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]] on recovery
[2016-03-30 14:05:20,437][WARN ][org.elasticsearch.indices.recovery] [node_t0] 0 Remote file corruption on node {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local], recovering name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]. local checksum OK
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
	at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
	at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:20,440][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] Got exception on recovery
RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:20,442][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] failing recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local], id [446]. Send shard failure: [true]
[2016-03-30 14:05:20,443][WARN ][org.elasticsearch.indices.cluster] [node_t2] [[test][0]] marking and sending shard failed due to [failed recovery]
RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]]; nested: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:260)
	at org.elasticsearch.indices.recovery.RecoveryTarget.access$1100(RecoveryTarget.java:68)
	at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:510)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:20,446][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] closing IndexOutput file [VerifyingIndexOutput(out=MockIndexOutputWrapper(FSIndexOutput(path="/Users/brijeshs/Documents/elasticsearch_sort_netbeans/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.recovery.RelocationIT_F7928424F380CF7C-001/tempDir-003/data/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-1872730973068554045]-HASH=[462EFF59DD44D]-cluster/nodes/2/indices/test/0/index/recovery.1459371920220.segments_2")))]
[2016-03-30 14:05:20,446][WARN ][org.elasticsearch.cluster.action.shard] [node_t0] [test][0] received shard failed for [test][0], node[KiCL2q9sQbOOLvzo3MxWkw], [R], v[3], s[INITIALIZING], a[id=wK4EWq8jRqK-JQARDfJQ-Q], unassigned_info[[reason=REPLICA_ADDED], at[2016-03-30T21:05:20.190Z]], indexUUID [ViysPQ8tQsijUsp5c1zZHQ], message [failed recovery], failure [RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]]; nested: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]; ]
RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]]; nested: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:260)
	at org.elasticsearch.indices.recovery.RecoveryTarget.access$1100(RecoveryTarget.java:68)
	at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:510)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:20,446][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920220.segments_2]
[2016-03-30 14:05:20,449][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920220._0_1.liv]
[2016-03-30 14:05:20,450][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920220._0.si]
[2016-03-30 14:05:20,452][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920220._1.cfs]
[2016-03-30 14:05:20,459][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920220._1.si]
[2016-03-30 14:05:20,470][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920220._0.cfe]
[2016-03-30 14:05:20,472][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920220._1.cfe]
[2016-03-30 14:05:20,482][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920220._0.cfs]
[2016-03-30 14:05:20,495][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] started recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local], id [447]
[2016-03-30 14:05:20,503][TRACE][org.elasticsearch.indices.recovery] [node_t1] collecting local files for [test][0] [447]
[2016-03-30 14:05:20,515][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] starting recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local]
[2016-03-30 14:05:20,516][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] starting recovery to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local], mark_as_relocated false
[2016-03-30 14:05:20,516][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [2] for recovery
[2016-03-30 14:05:20,556][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_0.cfe], does not exists in remote
[2016-03-30 14:05:20,557][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_0.si], does not exists in remote
[2016-03-30 14:05:20,557][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_0.cfs], does not exists in remote
[2016-03-30 14:05:20,557][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_1.cfs], does not exists in remote
[2016-03-30 14:05:20,557][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_1.cfe], does not exists in remote
[2016-03-30 14:05:20,557][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_1.si], does not exists in remote
[2016-03-30 14:05:20,557][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [segments_2], does not exists in remote
[2016-03-30 14:05:20,557][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_0_1.liv], does not exists in remote
[2016-03-30 14:05:20,557][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering_files [8] with total_size [8.3kb], reusing_files [0] with total_size [0b]
[2016-03-30 14:05:20,739][DEBUG][org.elasticsearch.indices.recovery] [node_t0] Failed to transfer file [name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]] on recovery
[2016-03-30 14:05:20,750][WARN ][org.elasticsearch.indices.recovery] [node_t0] 0 Remote file corruption on node {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local], recovering name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]. local checksum OK
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
	at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
	at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:20,752][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] Got exception on recovery
RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:20,753][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] failing recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local], id [447]. Send shard failure: [true]
[2016-03-30 14:05:20,753][WARN ][org.elasticsearch.indices.cluster] [node_t1] [[test][0]] marking and sending shard failed due to [failed recovery]
RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]]; nested: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:260)
	at org.elasticsearch.indices.recovery.RecoveryTarget.access$1100(RecoveryTarget.java:68)
	at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:510)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:20,758][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] closing IndexOutput file [VerifyingIndexOutput(out=MockIndexOutputWrapper(FSIndexOutput(path="/Users/brijeshs/Documents/elasticsearch_sort_netbeans/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.recovery.RelocationIT_F7928424F380CF7C-001/tempDir-003/data/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-1872730973068554045]-HASH=[462EFF59DD44D]-cluster/nodes/1/indices/test/0/index/recovery.1459371920495.segments_2")))]
[2016-03-30 14:05:20,758][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371920495._0.si]
[2016-03-30 14:05:20,758][WARN ][org.elasticsearch.cluster.action.shard] [node_t0] [test][0] received shard failed for [test][0], node[NjZvErxCTE6AtwBObgLE5w], [R], v[5], s[INITIALIZING], a[id=A6fRhUccRpWFNV2LoKnPQA], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-30T21:05:20.448Z], details[failed recovery, failure RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]]; nested: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]; ]], indexUUID [ViysPQ8tQsijUsp5c1zZHQ], message [failed recovery], failure [RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]]; nested: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]; ]
RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]]; nested: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:260)
	at org.elasticsearch.indices.recovery.RecoveryTarget.access$1100(RecoveryTarget.java:68)
	at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:510)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:20,759][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371920495._1.si]
[2016-03-30 14:05:20,761][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371920495._1.cfs]
[2016-03-30 14:05:20,761][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371920495._0.cfe]
[2016-03-30 14:05:20,773][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371920495._1.cfe]
[2016-03-30 14:05:20,784][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371920495._0.cfs]
[2016-03-30 14:05:20,786][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371920495.segments_2]
[2016-03-30 14:05:20,787][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371920495._0_1.liv]
[2016-03-30 14:05:20,828][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] started recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local], id [448]
[2016-03-30 14:05:20,829][TRACE][org.elasticsearch.indices.recovery] [node_t2] collecting local files for [test][0] [448]
[2016-03-30 14:05:20,850][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] starting recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local]
[2016-03-30 14:05:20,850][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] starting recovery to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local], mark_as_relocated false
[2016-03-30 14:05:20,850][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [2] for recovery
[2016-03-30 14:05:20,946][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_0.cfe], does not exists in remote
[2016-03-30 14:05:20,946][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_0.si], does not exists in remote
[2016-03-30 14:05:20,946][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_0.cfs], does not exists in remote
[2016-03-30 14:05:20,946][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_1.cfs], does not exists in remote
[2016-03-30 14:05:20,946][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_1.cfe], does not exists in remote
[2016-03-30 14:05:20,946][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_1.si], does not exists in remote
[2016-03-30 14:05:20,946][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [segments_2], does not exists in remote
[2016-03-30 14:05:20,946][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering [_0_1.liv], does not exists in remote
[2016-03-30 14:05:20,946][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]: recovering_files [8] with total_size [8.3kb], reusing_files [0] with total_size [0b]
[2016-03-30 14:05:21,013][DEBUG][org.elasticsearch.indices.recovery] [node_t0] Failed to transfer file [name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]] on recovery
[2016-03-30 14:05:21,013][WARN ][org.elasticsearch.indices.recovery] [node_t0] 0 Remote file corruption on node {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local], recovering name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]. local checksum OK
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
	at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
	at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:21,025][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] Got exception on recovery
RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:21,026][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] failing recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local], id [448]. Send shard failure: [true]
[2016-03-30 14:05:21,026][WARN ][org.elasticsearch.indices.cluster] [node_t2] [[test][0]] marking and sending shard failed due to [failed recovery]
RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]]; nested: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:260)
	at org.elasticsearch.indices.recovery.RecoveryTarget.access$1100(RecoveryTarget.java:68)
	at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:510)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:21,030][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] closing IndexOutput file [VerifyingIndexOutput(out=MockIndexOutputWrapper(FSIndexOutput(path="/Users/brijeshs/Documents/elasticsearch_sort_netbeans/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.recovery.RelocationIT_F7928424F380CF7C-001/tempDir-003/data/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-1872730973068554045]-HASH=[462EFF59DD44D]-cluster/nodes/2/indices/test/0/index/recovery.1459371920828.segments_2")))]
[2016-03-30 14:05:21,031][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920828._1.cfs]
[2016-03-30 14:05:21,031][WARN ][org.elasticsearch.cluster.action.shard] [node_t0] [test][0] received shard failed for [test][0], node[KiCL2q9sQbOOLvzo3MxWkw], [R], v[7], s[INITIALIZING], a[id=FshlN7D4Tl6zFyb4wQzAGQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-30T21:05:20.760Z], details[failed recovery, failure RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]]; nested: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]; ]], indexUUID [ViysPQ8tQsijUsp5c1zZHQ], message [failed recovery], failure [RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]]; nested: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]; ]
RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]]; nested: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:260)
	at org.elasticsearch.indices.recovery.RecoveryTarget.access$1100(RecoveryTarget.java:68)
	at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:510)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:21,042][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920828._0_1.liv]
[2016-03-30 14:05:21,043][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920828.segments_2]
[2016-03-30 14:05:21,053][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920828._0.cfs]
[2016-03-30 14:05:21,074][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920828._0.si]
[2016-03-30 14:05:21,085][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920828._0.cfe]
[2016-03-30 14:05:21,086][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920828._1.cfe]
[2016-03-30 14:05:21,090][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] cleaning temporary file [recovery.1459371920828._1.si]
[2016-03-30 14:05:21,121][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] started recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local], id [449]
[2016-03-30 14:05:21,122][TRACE][org.elasticsearch.indices.recovery] [node_t1] collecting local files for [test][0] [449]
[2016-03-30 14:05:21,143][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] starting recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local]
[2016-03-30 14:05:21,143][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] starting recovery to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local], mark_as_relocated false
[2016-03-30 14:05:21,144][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [2] for recovery
[2016-03-30 14:05:21,178][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_0.cfe], does not exists in remote
[2016-03-30 14:05:21,178][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_0.si], does not exists in remote
[2016-03-30 14:05:21,178][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_0.cfs], does not exists in remote
[2016-03-30 14:05:21,178][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_1.cfs], does not exists in remote
[2016-03-30 14:05:21,178][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_1.cfe], does not exists in remote
[2016-03-30 14:05:21,178][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_1.si], does not exists in remote
[2016-03-30 14:05:21,178][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [segments_2], does not exists in remote
[2016-03-30 14:05:21,178][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering [_0_1.liv], does not exists in remote
[2016-03-30 14:05:21,178][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]: recovering_files [8] with total_size [8.3kb], reusing_files [0] with total_size [0b]
[2016-03-30 14:05:21,344][DEBUG][org.elasticsearch.indices.recovery] [node_t0] Failed to transfer file [name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]] on recovery
[2016-03-30 14:05:21,344][WARN ][org.elasticsearch.indices.recovery] [node_t0] 0 Remote file corruption on node {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local], recovering name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]. local checksum OK
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
	at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
	at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
	at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:21,346][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] Got exception on recovery
RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:21,348][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] failing recovery from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local], id [449]. Send shard failure: [true]
[2016-03-30 14:05:21,349][WARN ][org.elasticsearch.indices.cluster] [node_t1] [[test][0]] marking and sending shard failed due to [failed recovery]
RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]]; nested: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:260)
	at org.elasticsearch.indices.recovery.RecoveryTarget.access$1100(RecoveryTarget.java:68)
	at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:510)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:21,352][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] closing IndexOutput file [VerifyingIndexOutput(out=MockIndexOutputWrapper(FSIndexOutput(path="/Users/brijeshs/Documents/elasticsearch_sort_netbeans/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.recovery.RelocationIT_F7928424F380CF7C-001/tempDir-003/data/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-1872730973068554045]-HASH=[462EFF59DD44D]-cluster/nodes/1/indices/test/0/index/recovery.1459371921121.segments_2")))]
[2016-03-30 14:05:21,352][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371921121._0.cfs]
[2016-03-30 14:05:21,352][WARN ][org.elasticsearch.cluster.action.shard] [node_t0] [test][0] received shard failed for [test][0], node[NjZvErxCTE6AtwBObgLE5w], [R], v[9], s[INITIALIZING], a[id=6n4gOekwTbeGP_v75VplFg], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-03-30T21:05:21.042Z], details[failed recovery, failure RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t2}{KiCL2q9sQbOOLvzo3MxWkw}{local}{local[479]}[mode=>local]]; nested: RemoteTransportException[[node_t2][local[479]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]; ]], indexUUID [ViysPQ8tQsijUsp5c1zZHQ], message [failed recovery], failure [RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]]; nested: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]; ]
RecoveryFailedException[[test][0]: Recovery failed from {node_t0}{KfmlrJoySiaNnFA0Pe6FTQ}{local}{local[476]}[mode=>local] into {node_t1}{NjZvErxCTE6AtwBObgLE5w}{local}{local[478]}[mode=>local]]; nested: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:260)
	at org.elasticsearch.indices.recovery.RecoveryTarget.access$1100(RecoveryTarget.java:68)
	at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:510)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[node_t1][local[478]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
Caused by: [test][[test][0]] RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:136)
	at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
	at org.elasticsearch.indices.recovery.RecoverySource.access$100(RecoverySource.java:46)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
	at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:123)
	at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: [test][[test][0]] RecoverFilesRecoveryException[Failed to transfer [8] files with total size of [8.3kb]]; nested: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]];
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:315)
	at org.elasticsearch.indices.recovery.RecoverySourceHandler.recoverToTarget(RecoverySourceHandler.java:134)
	... 9 more
Caused by: RemoteTransportException[[File corruption occurred on recovery but checksums are ok]]
	Suppressed: RemoteTransportException[[node_t0][local[476]][internal:index/shard/recovery/file_chunk]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))];
	Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=oqm3go actual=1ba9wfk (resource=name [segments_2], length [257], checksum [oqm3go], writtenBy [5.4.0]) (resource=VerifyingIndexOutput(segments_2))
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.readAndCompareChecksum(Store.java:1313)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeByte(Store.java:1292)
		at org.elasticsearch.index.store.Store$LuceneVerifyingIndexOutput.writeBytes(Store.java:1321)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:457)
		at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:421)
		at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
		at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
		at java.lang.Thread.run(Thread.java:745)
[2016-03-30 14:05:21,353][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371921121._0.cfe]
[2016-03-30 14:05:21,354][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371921121._1.cfe]
[2016-03-30 14:05:21,355][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371921121._1.cfs]
[2016-03-30 14:05:21,358][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371921121.segments_2]
[2016-03-30 14:05:21,360][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371921121._0_1.liv]
[2016-03-30 14:05:21,365][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371921121._0.si]
[2016-03-30 14:05:21,371][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] cleaning temporary file [recovery.1459371921121._1.si]
[2016-03-30 14:05:21,716][DEBUG][org.elasticsearch.indices.recovery] [node_t0] using max_bytes_per_sec[40mb], concurrent_streams [6], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:05:21,980][DEBUG][org.elasticsearch.indices.recovery] [node_t1] using max_bytes_per_sec[40mb], concurrent_streams [6], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [false]
[2016-03-30 14:05:22,029][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] started recovery from {node_t0}{Uj5uMn0ETSCCG26ClNuOUQ}{local}{local[482]}[mode=>local], id [450]
[2016-03-30 14:05:22,029][TRACE][org.elasticsearch.indices.recovery] [node_t1] collecting local files for [test][0] [450]
[2016-03-30 14:05:22,048][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] starting recovery from {node_t0}{Uj5uMn0ETSCCG26ClNuOUQ}{local}{local[482]}[mode=>local]
[2016-03-30 14:05:22,048][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] starting recovery to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local], mark_as_relocated false
[2016-03-30 14:05:22,049][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [1] for recovery
[2016-03-30 14:05:22,069][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]: recovering [segments_1], does not exists in remote
[2016-03-30 14:05:22,069][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]: recovering_files [1] with total_size [130b], reusing_files [0] with total_size [0b]
[2016-03-30 14:05:22,135][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]: prepare remote engine for translog
[2016-03-30 14:05:22,188][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]: remote engine start took [52.6ms]
[2016-03-30 14:05:22,188][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]: took [0s]
[2016-03-30 14:05:22,188][TRACE][org.elasticsearch.indices.recovery] [node_t0] snapshot translog for recovery. current size is [0]
[2016-03-30 14:05:22,188][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]: sending transaction log operations
[2016-03-30 14:05:22,188][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] no translog operations to send to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]
[2016-03-30 14:05:22,189][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] sent final batch of [0][0b] (total: [0]) translog operations to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]
[2016-03-30 14:05:22,189][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]: took [317.8micros]
[2016-03-30 14:05:22,189][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]
[2016-03-30 14:05:22,189][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t1}{H_2DyR3hTtGNtxhp1NXOOw}{local}{local[484]}[mode=>local]: took [437.3micros]
[2016-03-30 14:05:22,189][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] marking recovery from {node_t0}{Uj5uMn0ETSCCG26ClNuOUQ}{local}{local[482]}[mode=>local] as done, id [450]
[2016-03-30 14:05:22,190][TRACE][org.elasticsearch.indices.recovery] [node_t1] [test][0] recovery completed from {node_t0}{Uj5uMn0ETSCCG26ClNuOUQ}{local}{local[482]}[mode=>local], took[160ms]
   phase1: recovered_files [1] with total_size of [130b], took [0s], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [52ms]
         : recovered [0] transaction log operations, took [0s]

[2016-03-30 14:05:22,276][DEBUG][org.elasticsearch.indices.recovery] [node_t2] using max_bytes_per_sec[158mb], concurrent_streams [6], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2016-03-30 14:05:22,349][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] started recovery from {node_t0}{Uj5uMn0ETSCCG26ClNuOUQ}{local}{local[482]}[mode=>local], id [451]
[2016-03-30 14:05:22,350][TRACE][org.elasticsearch.indices.recovery] [node_t2] collecting local files for [test][0] [451]
[2016-03-30 14:05:22,364][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] starting recovery from {node_t0}{Uj5uMn0ETSCCG26ClNuOUQ}{local}{local[482]}[mode=>local]
[2016-03-30 14:05:22,365][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] starting recovery to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local], mark_as_relocated false
[2016-03-30 14:05:22,366][TRACE][org.elasticsearch.indices.recovery] [node_t0] captured translog id [1] for recovery
[2016-03-30 14:05:22,423][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]: recovering [segments_1], does not exists in remote
[2016-03-30 14:05:22,423][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]: recovering_files [1] with total_size [130b], reusing_files [0] with total_size [0b]
[2016-03-30 14:05:22,496][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]: prepare remote engine for translog
[2016-03-30 14:05:22,560][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]: remote engine start took [63.9ms]
[2016-03-30 14:05:22,560][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase1] to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]: took [0s]
[2016-03-30 14:05:22,560][TRACE][org.elasticsearch.indices.recovery] [node_t0] snapshot translog for recovery. current size is [25]
[2016-03-30 14:05:22,561][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]: sending transaction log operations
[2016-03-30 14:05:22,572][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] sent final batch of [25][1.5kb] (total: [25]) translog operations to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]
[2016-03-30 14:05:22,572][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] recovery [phase2] to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]: took [11.9ms]
[2016-03-30 14:05:22,573][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]
[2016-03-30 14:05:23,037][TRACE][org.elasticsearch.indices.recovery] [node_t0] [test][0] finalizing recovery to {node_t2}{owtMUNQWQ4imlb1EPr7UYg}{local}{local[485]}[mode=>local]: took [464.7ms]
[2016-03-30 14:05:23,038][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] marking recovery from {node_t0}{Uj5uMn0ETSCCG26ClNuOUQ}{local}{local[482]}[mode=>local] as done, id [451]
[2016-03-30 14:05:23,038][TRACE][org.elasticsearch.indices.recovery] [node_t2] [test][0] recovery completed from {node_t0}{Uj5uMn0ETSCCG26ClNuOUQ}{local}{local[482]}[mode=>local], took[688ms]
   phase1: recovered_files [1] with total_size of [130b], took [0s], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: start took [63ms]
         : recovered [25] transaction log operations, took [11ms]

[2016-03-30 14:05:51,059][WARN ][org.elasticsearch.test.transport] [node_s1] Transport response handler not found of id [336]
[2016-03-30 14:06:28,609][WARN ][org.elasticsearch.test.transport] [node_s1] Transport response handler not found of id [468]
